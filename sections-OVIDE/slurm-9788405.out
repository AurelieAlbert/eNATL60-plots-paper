/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:39434'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:40500'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:38215'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:33772'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:36952'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:33278'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:41199'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:46098'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:40083'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:37862'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:33965'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:41137'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:46497'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:39308'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:40418'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:39471'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:44379'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:36659'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:33140'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:41558'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:41327'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:40321'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:45546'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:41111'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:43016'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:36903'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:38938'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.10:46043'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:34495
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:41883
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:34495
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:41883
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:46054
distributed.worker - INFO -              nanny at:         172.30.10.10:33772
distributed.worker - INFO -              nanny at:         172.30.10.10:36952
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:46054
distributed.worker - INFO -              bokeh at:         172.30.10.10:42961
distributed.worker - INFO -              bokeh at:         172.30.10.10:42059
distributed.worker - INFO -              nanny at:         172.30.10.10:41558
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO -              bokeh at:         172.30.10.10:32990
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bd8emb65
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rwiv6pnp
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-b8ne_z3b
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:44291
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:44291
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:33384
distributed.worker - INFO -              nanny at:         172.30.10.10:40083
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:33384
distributed.worker - INFO -              bokeh at:         172.30.10.10:35266
distributed.worker - INFO -              nanny at:         172.30.10.10:33278
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO -              bokeh at:         172.30.10.10:42394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wz6zlzlp
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ey4ctum9
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:37513
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:37513
distributed.worker - INFO -              nanny at:         172.30.10.10:33140
distributed.worker - INFO -              bokeh at:         172.30.10.10:33023
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-q_c5rcf_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:37687
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:37687
distributed.worker - INFO -              nanny at:         172.30.10.10:44379
distributed.worker - INFO -              bokeh at:         172.30.10.10:36169
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_2gt17sc
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:44761
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:44761
distributed.worker - INFO -              nanny at:         172.30.10.10:39308
distributed.worker - INFO -              bokeh at:         172.30.10.10:34155
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wljzkbhm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:41617
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:41617
distributed.worker - INFO -              nanny at:         172.30.10.10:36903
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -              bokeh at:         172.30.10.10:43557
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8xt6y20d
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:36813
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:36813
distributed.worker - INFO -              nanny at:         172.30.10.10:41199
distributed.worker - INFO -              bokeh at:         172.30.10.10:41497
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-uxrm93xc
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:41722
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:41722
distributed.worker - INFO -              nanny at:         172.30.10.10:41111
distributed.worker - INFO -              bokeh at:         172.30.10.10:39443
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dcloc3lc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:43763
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:43763
distributed.worker - INFO -              nanny at:         172.30.10.10:36659
distributed.worker - INFO -              bokeh at:         172.30.10.10:43315
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:39429
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:39429
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gw925jdh
distributed.worker - INFO -              nanny at:         172.30.10.10:46098
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:         172.30.10.10:42732
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:39105
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-oawnd1y2
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:39105
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:         172.30.10.10:46497
distributed.worker - INFO -              bokeh at:         172.30.10.10:45216
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-o9_ii1p4
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:46117
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:46117
distributed.worker - INFO -              nanny at:         172.30.10.10:40500
distributed.worker - INFO -              bokeh at:         172.30.10.10:46047
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qp3zal0a
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:37491
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:37491
distributed.worker - INFO -              nanny at:         172.30.10.10:45546
distributed.worker - INFO -              bokeh at:         172.30.10.10:37301
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-t9nqvwa2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:38059
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:38059
distributed.worker - INFO -              nanny at:         172.30.10.10:37862
distributed.worker - INFO -              bokeh at:         172.30.10.10:37202
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0u59a76w
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:41657
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:41657
distributed.worker - INFO -              nanny at:         172.30.10.10:41327
distributed.worker - INFO -              bokeh at:         172.30.10.10:42476
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-eforqwav
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:39944
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:39944
distributed.worker - INFO -              nanny at:         172.30.10.10:41137
distributed.worker - INFO -              bokeh at:         172.30.10.10:40645
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pt7lzknl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:38561
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:38561
distributed.worker - INFO -              nanny at:         172.30.10.10:46043
distributed.worker - INFO -              bokeh at:         172.30.10.10:37177
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nvox7osb
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:35698
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:35698
distributed.worker - INFO -              nanny at:         172.30.10.10:38938
distributed.worker - INFO -              bokeh at:         172.30.10.10:41323
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3jo2r9jq
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:42339
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:42339
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:         172.30.10.10:40418
distributed.worker - INFO -              bokeh at:         172.30.10.10:35204
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ww0647zc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:38328
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:38328
distributed.worker - INFO -              nanny at:         172.30.10.10:38215
distributed.worker - INFO -              bokeh at:         172.30.10.10:40270
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9sjh6mdl
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:38177
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:38177
distributed.worker - INFO -              nanny at:         172.30.10.10:39434
distributed.worker - INFO -              bokeh at:         172.30.10.10:43678
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-1lg8h71o
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:41264
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:41264
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO -              nanny at:         172.30.10.10:33965
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -              bokeh at:         172.30.10.10:39199
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-o6sw6wbr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:42860
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:42860
distributed.worker - INFO -              nanny at:         172.30.10.10:43016
distributed.worker - INFO -              bokeh at:         172.30.10.10:46869
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-s_7nx1jh
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:39978
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:39978
distributed.worker - INFO -              nanny at:         172.30.10.10:40321
distributed.worker - INFO -              bokeh at:         172.30.10.10:46625
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-d_skdto0
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.10:32854
distributed.worker - INFO -          Listening to:   tcp://172.30.10.10:32854
distributed.worker - INFO -              nanny at:         172.30.10.10:39471
distributed.worker - INFO -              bokeh at:         172.30.10.10:33767
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zq8tu413
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.3:46204
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 157.86 MB from 3822 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 157.77 MB from 3796 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 157.99 MB from 2543 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 157.85 MB from 3735 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 316.01 MB from 1863 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.utils_perf - INFO - full garbage collection released 157.95 MB from 3988 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.utils_perf - INFO - full garbage collection released 52.23 MB from 4170 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.3:46204
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:41617
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:36903'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:44291
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:39978
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:41883
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:42339
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:46117
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:46054
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:37687
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:40083'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:38561
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:40321'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:36952'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:40418'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:41264
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:39429
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:42860
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:44761
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:43763
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:38177
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:40500'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:41657
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:33384
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:41558'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:39105
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:37513
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:44379'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:36813
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:46043'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:33965'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:35698
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:41722
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:34495
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:46098'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:39308'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:43016'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:38059
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:36659'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:39434'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:37491
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:39944
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:41327'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:33278'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:46497'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:32854
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:33140'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.10:38328
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:41199'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:38938'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:41111'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:33772'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:37862'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:41137'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:45546'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:39471'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.10:38215'
distributed.dask_worker - INFO - End worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
