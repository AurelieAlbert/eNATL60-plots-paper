/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:44169'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:39243'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:34077'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:39808'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:37310'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:45889'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:33533'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:44950'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:38371'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:45594'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:39043'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:35746'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:39150'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:40565'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:36745'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:37193'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:35584'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:40552'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:44633'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:42601'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:43449'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:45782'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:36721'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:46740'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:37747'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:34290'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:44377'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.81:44861'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:40063
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:45534
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:36116
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:44225
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:38276
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:42898
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:39611
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:42383
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:38353
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:39572
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:40063
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:34699
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:45534
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:36116
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:44225
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:38276
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:42898
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:39611
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:42383
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:44012
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:38353
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:39404
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:46827
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:39572
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:42425
distributed.worker - INFO -              nanny at:          172.30.6.81:42601
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:41672
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:39382
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:41570
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:34699
distributed.worker - INFO -              nanny at:          172.30.6.81:40565
distributed.worker - INFO -              nanny at:          172.30.6.81:37193
distributed.worker - INFO -              nanny at:          172.30.6.81:37310
distributed.worker - INFO -              nanny at:          172.30.6.81:35746
distributed.worker - INFO -              nanny at:          172.30.6.81:45594
distributed.worker - INFO -              nanny at:          172.30.6.81:35584
distributed.worker - INFO -              nanny at:          172.30.6.81:44950
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:44012
distributed.worker - INFO -              nanny at:          172.30.6.81:39150
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:39404
distributed.worker - INFO -              nanny at:          172.30.6.81:36745
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:46827
distributed.worker - INFO -              bokeh at:          172.30.6.81:44356
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:42425
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:41672
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:39382
distributed.worker - INFO -              nanny at:          172.30.6.81:45889
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:41570
distributed.worker - INFO -              bokeh at:          172.30.6.81:36988
distributed.worker - INFO -              bokeh at:          172.30.6.81:45904
distributed.worker - INFO -              bokeh at:          172.30.6.81:41802
distributed.worker - INFO -              bokeh at:          172.30.6.81:42508
distributed.worker - INFO -              bokeh at:          172.30.6.81:38857
distributed.worker - INFO -              bokeh at:          172.30.6.81:36563
distributed.worker - INFO -              bokeh at:          172.30.6.81:44083
distributed.worker - INFO -              nanny at:          172.30.6.81:34077
distributed.worker - INFO -              bokeh at:          172.30.6.81:34022
distributed.worker - INFO -              bokeh at:          172.30.6.81:43390
distributed.worker - INFO -              nanny at:          172.30.6.81:39808
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO -              nanny at:          172.30.6.81:43449
distributed.worker - INFO -              nanny at:          172.30.6.81:40552
distributed.worker - INFO -              bokeh at:          172.30.6.81:38717
distributed.worker - INFO -              nanny at:          172.30.6.81:39043
distributed.worker - INFO -              nanny at:          172.30.6.81:46740
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO -              nanny at:          172.30.6.81:38371
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO -              bokeh at:          172.30.6.81:43941
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.6.81:41615
distributed.worker - INFO -              bokeh at:          172.30.6.81:45799
distributed.worker - INFO -              bokeh at:          172.30.6.81:46091
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO -              bokeh at:          172.30.6.81:43872
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.6.81:33168
distributed.worker - INFO -              bokeh at:          172.30.6.81:39220
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:38307
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:38307
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:46715
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -              nanny at:          172.30.6.81:39243
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:46715
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ahykocq3
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0bgtmh10
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-cz2pcz6m
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-oqv8o5ht
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0onw4xu8
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-br8irfzs
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fauufbvb
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-g_f9plty
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -              nanny at:          172.30.6.81:45782
distributed.worker - INFO -              bokeh at:          172.30.6.81:34424
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hvibr4g0
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pk2qko3r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xiw9gi4j
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:          172.30.6.81:45338
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-db0g9mor
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kj0q8dus
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-w6a6uk7f
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-xdscym8a
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8an7d8ru
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gvdbckcl
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-oehv5hb7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-t02jzwid
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rbframgt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:36279
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:36279
distributed.worker - INFO -              nanny at:          172.30.6.81:37747
distributed.worker - INFO -              bokeh at:          172.30.6.81:34550
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:35751
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ujbmtak3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:35751
distributed.worker - INFO -              nanny at:          172.30.6.81:34290
distributed.worker - INFO -              bokeh at:          172.30.6.81:45611
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-gonyw5_v
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:33303
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:33303
distributed.worker - INFO -              nanny at:          172.30.6.81:36721
distributed.worker - INFO -              bokeh at:          172.30.6.81:39177
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:33734
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:33734
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -              nanny at:          172.30.6.81:44861
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hl61vwvy
distributed.worker - INFO -              bokeh at:          172.30.6.81:45659
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-o48f0lq8
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:35596
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:35596
distributed.worker - INFO -              nanny at:          172.30.6.81:33533
distributed.worker - INFO -              bokeh at:          172.30.6.81:43323
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-g2k0rfph
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:34015
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:34015
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:          172.30.6.81:44377
distributed.worker - INFO -              bokeh at:          172.30.6.81:39879
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:45129
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bofnh90f
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:45129
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:          172.30.6.81:44633
distributed.worker - INFO -              bokeh at:          172.30.6.81:39442
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-a4m86uo9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:45002
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:45002
distributed.worker - INFO -              nanny at:          172.30.6.81:44169
distributed.worker - INFO -              bokeh at:          172.30.6.81:37194
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-cyms56mz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - WARNING - Worker process 25937 was killed by signal 11
distributed.nanny - WARNING - Restarting worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:    tcp://172.30.6.81:41399
distributed.worker - INFO -          Listening to:    tcp://172.30.6.81:41399
distributed.worker - INFO -              nanny at:          172.30.6.81:44950
distributed.worker - INFO -              bokeh at:          172.30.6.81:46237
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dbikt11g
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.utils_perf - INFO - full garbage collection released 41.11 MB from 1990 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 41.09 MB from 2073 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:41399
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.81:44950'
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:44012
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:35596
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:34699
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:41570
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:35751
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:45129
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:33734
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:45002
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:39404
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:36279
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:39572
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:41672
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:33303
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:38307
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:45534
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:38353
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:34015
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:46827
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:36116
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:44225
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:46715
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:39611
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:42898
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:38276
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:40063
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:42425
distributed.worker - INFO - Stopping worker at tcp://172.30.6.81:39382
distributed.dask_worker - INFO - End worker
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-3, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-14, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-21, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-28, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-13, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-20, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-2, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-27, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-12, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-1, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-19, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-26, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-11, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-4, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-6, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-25, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-18, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-10, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-17, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-24, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-9, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-22, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-5, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-23, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-15, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-16, started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-7, started daemon)>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
