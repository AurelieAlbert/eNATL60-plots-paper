/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:35392'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:44333'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:35610'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:43466'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:43293'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:38222'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:41503'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:38880'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:36046'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:42317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:37988'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:37075'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:35489'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:36228'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:38388'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:36838'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:41693'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:43754'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:45051'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:40698'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:37660'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:39942'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:41301'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:40962'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:35617'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:38506'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:40740'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.10.33:44685'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:37918
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:37918
distributed.worker - INFO -              nanny at:         172.30.10.33:36046
distributed.worker - INFO -              bokeh at:         172.30.10.33:39494
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-acr7bosx
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:38660
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:38660
distributed.worker - INFO -              nanny at:         172.30.10.33:42317
distributed.worker - INFO -              bokeh at:         172.30.10.33:35630
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-e6znq15_
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:45761
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:45761
distributed.worker - INFO -              nanny at:         172.30.10.33:36228
distributed.worker - INFO -              bokeh at:         172.30.10.33:36719
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qojzucs7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:37084
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:37084
distributed.worker - INFO -              nanny at:         172.30.10.33:38880
distributed.worker - INFO -              bokeh at:         172.30.10.33:36978
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-u7zorig4
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:36394
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:36394
distributed.worker - INFO -              nanny at:         172.30.10.33:43293
distributed.worker - INFO -              bokeh at:         172.30.10.33:45240
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dhpwa1a3
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:37125
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:37125
distributed.worker - INFO -              nanny at:         172.30.10.33:41301
distributed.worker - INFO -              bokeh at:         172.30.10.33:34768
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rmunfue4
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:44749
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:44749
distributed.worker - INFO -              nanny at:         172.30.10.33:38506
distributed.worker - INFO -              bokeh at:         172.30.10.33:39173
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-k8vz1ixr
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:44259
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:44259
distributed.worker - INFO -              nanny at:         172.30.10.33:44333
distributed.worker - INFO -              bokeh at:         172.30.10.33:36949
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kc51vlcp
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:46524
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:46524
distributed.worker - INFO -              nanny at:         172.30.10.33:39942
distributed.worker - INFO -              bokeh at:         172.30.10.33:44826
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-he02_7zt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:42989
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:42989
distributed.worker - INFO -              nanny at:         172.30.10.33:40740
distributed.worker - INFO -              bokeh at:         172.30.10.33:37121
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:37709
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9z27r9jv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:37709
distributed.worker - INFO -              nanny at:         172.30.10.33:35489
distributed.worker - INFO -              bokeh at:         172.30.10.33:41459
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kbxf94ni
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:33673
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:33673
distributed.worker - INFO -              nanny at:         172.30.10.33:41503
distributed.worker - INFO -              bokeh at:         172.30.10.33:39321
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3_xunc_9
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:34029
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:34029
distributed.worker - INFO -              nanny at:         172.30.10.33:36838
distributed.worker - INFO -              bokeh at:         172.30.10.33:40528
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-fvoc6fn5
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:37971
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:37971
distributed.worker - INFO -              nanny at:         172.30.10.33:41693
distributed.worker - INFO -              bokeh at:         172.30.10.33:35693
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-64zdxngh
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:38558
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:38558
distributed.worker - INFO -              nanny at:         172.30.10.33:40698
distributed.worker - INFO -              bokeh at:         172.30.10.33:39811
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:42354
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:42354
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -              nanny at:         172.30.10.33:40962
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-n6_fb6m4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:         172.30.10.33:36500
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-08bfblm8
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:41325
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:41325
distributed.worker - INFO -              nanny at:         172.30.10.33:44685
distributed.worker - INFO -              bokeh at:         172.30.10.33:34344
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8gtui257
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:41720
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:41720
distributed.worker - INFO -              nanny at:         172.30.10.33:43466
distributed.worker - INFO -              bokeh at:         172.30.10.33:39132
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-es_8wnx8
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:40718
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:40718
distributed.worker - INFO -              nanny at:         172.30.10.33:37988
distributed.worker - INFO -              bokeh at:         172.30.10.33:35810
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-y5nsfgn_
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:41279
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:41279
distributed.worker - INFO -              nanny at:         172.30.10.33:38388
distributed.worker - INFO -              bokeh at:         172.30.10.33:42242
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ad0y1dj9
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:41545
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:41545
distributed.worker - INFO -              nanny at:         172.30.10.33:35610
distributed.worker - INFO -              bokeh at:         172.30.10.33:36030
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pyz17bzo
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:45988
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:45988
distributed.worker - INFO -              nanny at:         172.30.10.33:37660
distributed.worker - INFO -              bokeh at:         172.30.10.33:38800
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-b2p1xtbo
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:38662
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:38662
distributed.worker - INFO -              nanny at:         172.30.10.33:37075
distributed.worker - INFO -              bokeh at:         172.30.10.33:42921
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hzt8smbf
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:43027
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:43027
distributed.worker - INFO -              nanny at:         172.30.10.33:43754
distributed.worker - INFO -              bokeh at:         172.30.10.33:41201
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bofuqbvu
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:43756
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:43756
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:         172.30.10.33:38222
distributed.worker - INFO -              bokeh at:         172.30.10.33:42500
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-l5lbgm3z
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:44649
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:44649
distributed.worker - INFO -              nanny at:         172.30.10.33:35617
distributed.worker - INFO -              bokeh at:         172.30.10.33:45401
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-oa30i26x
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:35581
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:35581
distributed.worker - INFO -              nanny at:         172.30.10.33:45051
distributed.worker - INFO -              bokeh at:         172.30.10.33:41823
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-nj8przx2
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:37524
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:37524
distributed.worker - INFO -              nanny at:         172.30.10.33:35392
distributed.worker - INFO -              bokeh at:         172.30.10.33:44917
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7j5ebxu3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils - ERROR - Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py", line 713, in log_errors
    yield
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1994, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
distributed.utils - ERROR - Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py", line 713, in log_errors
    yield
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1923, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1994, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b69d7dcd198>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1923, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1994, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
distributed.utils - ERROR - Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py", line 713, in log_errors
    yield
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1994, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
distributed.utils - ERROR - Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py", line 713, in log_errors
    yield
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1923, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1994, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b69d7dcd198>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1923, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1994, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
distributed.utils - ERROR - Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py", line 713, in log_errors
    yield
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1994, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
distributed.utils - ERROR - Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py", line 713, in log_errors
    yield
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1923, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1994, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b69d7dcd198>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1923, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1994, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
distributed.utils - ERROR - Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py", line 713, in log_errors
    yield
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1994, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
distributed.utils - ERROR - Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py", line 713, in log_errors
    yield
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1923, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1994, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2b69d7dcd198>>, <Future finished exception=OSError("Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time")>)
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 743, in _run_callback
    ret = callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 767, in _discard_future_result
    future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1923, in gather_dep
    yield self.query_who_has(dep)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1994, in query_who_has
    response = yield self.scheduler.who_has(keys=deps)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 736, in send_recv_from_rpc
    comm = yield self.pool.connect(self.addr)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.100.4:34922' after 10 s: connect() didn't finish in time
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.33:50644 remote=tcp://172.30.100.4:34922>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.33:50664 remote=tcp://172.30.100.4:34922>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.33:50672 remote=tcp://172.30.100.4:34922>
distributed.utils_perf - INFO - full garbage collection released 99.77 MB from 1302 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 206.41 MB from 862 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.10.33:50666 remote=tcp://172.30.100.4:34922>
distributed.utils_perf - INFO - full garbage collection released 123.25 MB from 787 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 41.01 MB from 2561 reference cycles (threshold: 10.00 MB)
distributed.nanny - WARNING - Worker process 35752 was killed by signal 11
distributed.nanny - WARNING - Restarting worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.10.33:39226
distributed.worker - INFO -          Listening to:   tcp://172.30.10.33:39226
distributed.worker - INFO -              nanny at:         172.30.10.33:36228
distributed.worker - INFO -              bokeh at:         172.30.10.33:45461
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-r8sfd7nq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.4:34922
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
Exception ignored in: <function CachingFileManager.__del__ at 0x2b6a7848b950>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/backends/file_manager.py", line 243, in __del__
    self.close(needs_lock=False)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/backends/file_manager.py", line 221, in close
    file.close()
  File "netCDF4/_netCDF4.pyx", line 2453, in netCDF4._netCDF4.Dataset.close
  File "netCDF4/_netCDF4.pyx", line 2417, in netCDF4._netCDF4.Dataset._close
  File "netCDF4/_netCDF4.pyx", line 1857, in netCDF4._netCDF4._ensure_nc_success
RuntimeError: NetCDF: HDF error
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.utils_perf - INFO - full garbage collection released 371.55 MB from 3636 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.utils_perf - INFO - full garbage collection released 268.13 MB from 3885 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.4:34922
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:39226
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:36228'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:35581
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:46524
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:37971
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:45051'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:44649
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:41325
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:43027
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:44259
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:41545
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:33673
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:39942'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:44749
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:42989
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:41693'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:37918
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:38558
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:35617'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:37084
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:37524
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:43756
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:41720
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:38662
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:37709
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:40718
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:43754'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:44685'
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:45988
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:36394
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:41279
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:34029
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:42354
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:37125
distributed.worker - INFO - Stopping worker at tcp://172.30.10.33:38660
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:41503'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:35610'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:44333'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:38506'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:40740'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:36046'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:40698'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:35489'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:38222'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:38880'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:43466'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:35392'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:37988'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:37075'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:37660'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:43293'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:36838'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:38388'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:40962'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:41301'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.10.33:42317'
distributed.dask_worker - INFO - End worker
distributed.process - WARNING - reaping stray process <ForkServerProcess(ForkServerProcess-20, started daemon)>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
