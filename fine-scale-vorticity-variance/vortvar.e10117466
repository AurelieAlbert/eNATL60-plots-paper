load module openmpi-icc/2.0.1 (openmpi)
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:  tcp://172.30.0.181:42447
distributed.scheduler - INFO -   dashboard at:                     :8787
distributed.scheduler - INFO - Receive client connection: Client-60e2cb02-861c-11ea-81b6-0800383bad01
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:45177
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:45177
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:44800
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:44800
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-rvnent89
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-9elz7tta
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:34395
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:34395
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:46074
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:46074
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-eg5dxgv7
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-bz4hdmdi
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:36305
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:36305
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-46ksf7vo
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:45166
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:45166
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:44423
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:44423
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:40015
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:40015
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:37800
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:37800
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-p5344apn
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:40463
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:40463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3mi9zf21
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:37587
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:37587
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:45750
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:45750
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-blf5672m
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:40776
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:40776
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-w912uu3d
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:43058
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:43058
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-p0v9dsst
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:43805
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:37838
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:37838
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:38058
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:38058
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:43964
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:36781
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:36781
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:42103
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:42103
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:43805
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:43273
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:43273
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:43964
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:43185
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:43185
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:34314
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:34314
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3ffaa4t5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-yzrlyr3a
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:33828
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:33828
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:41694
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:41694
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:41639
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:41639
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-nbdm0ktx
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-2n5i8wt0
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:38457
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:38457
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3hsrnt9s
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:46571
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:46571
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-74eov0hm
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-yx8_su5f
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-i1qxzo3v
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-af889tg1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-k71i5l9b
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:40929
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:40929
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:40869
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:40869
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:37008
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:37008
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:38367
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:38367
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:43099
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:43099
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-5h1wgkni
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:42115
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:42115
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:42999
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:42999
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-_slpd2_k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-mh1grj3a
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:42463
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:42463
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:38871
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:38871
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:40526
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:38603
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:38603
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:42340
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:42340
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-v7l7e6za
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:39785
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:39785
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-e8n5oqcs
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:41063
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:41063
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:35988
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:35988
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:40526
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:44769
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:44769
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-a757c739
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:33971
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:33971
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-9rroph3f
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-tkal7rme
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:32869
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:32869
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:44192
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:44192
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-c20n5zl5
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:45287
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:45287
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-g26k14qd
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-gelxp2in
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-w0qj2ck_
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-uhuhvrk7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:36411
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:36411
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-7jcd_ty6
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-g6jmx4vv
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:40157
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:40157
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-w6kpi4fq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:42279
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:42279
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:37847
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:37847
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-t1buj3ej
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-9yq3qi1q
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:43960
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-_p6yqldn
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3va2f4c_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-e1znijrh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:43960
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-_4ekqnfe
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-8b6_bni1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-l_y4yieb
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:45253
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-zfqgat50
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:45563
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-b2a0y4aq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-fjh3mxlb
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:45563
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-mtza6elc
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:45253
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-cpsu9kcj
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:36016
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:36016
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-8bzq3s7_
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-6kte800u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:37513
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-7s7juome
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:37513
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-an_y7qs2
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-94_udg8e
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:41984
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:41984
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:36958
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:36958
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-296xozui
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:43066
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:43066
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:42130
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:42130
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:43637
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:43637
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:38820
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:38820
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:42985
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:42985
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:37634
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:37634
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-hcxcxqlc
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:35828
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:35828
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-un1n5ec6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:36695
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-esxa0udk
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:45738
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:45738
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-h4laivlw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:36363
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:36363
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-eye_phd4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:44607
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:44607
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:36695
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-9rts2lns
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-1ypks7tq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-q4mpte7r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-z7g8j8io
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:42028
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:42028
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:33963
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:44274
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:44274
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:37865
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:37865
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ph3iigiv
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:39892
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:33963
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-s5yab93l
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:36886
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:36886
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-96dml4o7
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:39761
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:39761
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:42666
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:42666
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:33923
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:38563
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:38563
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:38926
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:38926
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-f92ovopw
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:39225
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:33652
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:33652
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3k028d99
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:34939
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:34939
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:33923
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:34467
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:34467
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:37401
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:39892
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-rlhl0ikm
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:39225
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:40848
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:40848
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-91mrdegq
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:37112
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:37112
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-v0qnod11
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:45158
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:45158
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:37401
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-85fo4n5e
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:43874
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:43874
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:39226
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-d27__ami
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:42071
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-r47_4e31
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-scdnunp7
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-pxjis11n
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:40833
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:39226
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:40833
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-5cljiink
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:40820
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:40820
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:42071
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-a9pbnb4d
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ro__m7nr
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:34200
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:34200
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-nqiqzii0
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-wyl9df8h
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-wdg_segx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-qb8tcomc
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-0b_t2xef
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ns52ddry
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-srrronrv
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-rj7jof2e
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:41514
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:41514
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-wikbtx3t
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-xmcv_7kb
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-lcwgn9lu
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-g15vj3gf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:37389
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ocyu8j_6
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-76vifj9c
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-e8470g9d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:37389
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ozkn2oej
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:39466
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:35418
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:39730
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:39466
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:35418
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-molu3tti
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:39730
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-c7ju3srj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-w3lrwuip
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-y466cr0w
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:39425
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:45002
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:39425
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:45002
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-bfc2zfqs
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-k5_8re37
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:44846
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:44846
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-daldfy4h
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:45238
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:45238
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-7in7a556
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:33275
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:33275
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.181:40157
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.181:40157
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-vsdrqqwm
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3ysi3fuu
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:46164
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:46164
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:44294
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:44294
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:37460
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:37460
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:42489
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:42489
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-psmu9vnx
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:33407
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:33407
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-roeyfflx
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-qxvavpxy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-yef3bcxb
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-jy6ixlza
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:38917
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:38917
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-i0xk1ugk
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:41181
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:43398
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:41181
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:43398
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-gjgcpuw5
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-oumfb6jn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:39315
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:39315
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:38640
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-0hiia9v7
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:38640
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-p7b_vbee
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:44192', name: 15, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.191:43673
distributed.worker - INFO -          Listening to:   tcp://172.30.0.191:43673
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-5py_7b2t
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:44192
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:36411', name: 12, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:36411
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:42279', name: 2, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:42279
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:43525
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:43525
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:37847', name: 7, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:35257
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:36135
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:36135
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:35257
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:37847
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:36016', name: 6, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-72_tcywa
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-sks9tdq5
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-33uriygj
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:36016
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:45563', name: 21, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:45563
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:42130', name: 3, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:42130
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:39231
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:39231
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:42985', name: 18, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:34103
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:34103
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-o4gae6ch
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-e13916qg
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:42985
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:35828', name: 23, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:35828
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:43684
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:43684
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:43637', name: 13, memory: 0, processing: 0>
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-9zu4d4my
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:46337
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:32973
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:32973
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:43637
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:46337
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:38820', name: 16, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-_3ig105v
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-lyww5o_r
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:38820
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:36363', name: 17, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:36363
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:45738', name: 11, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:45738
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:37865', name: 19, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:37865
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:36486
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:36486
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:37634', name: 22, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-w6rf1tw8
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:34377
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:34377
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-5n7sdxnp
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:37634
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:42665
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:45238', name: 9, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:42665
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-8q0u82d6
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:45238
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:33275', name: 20, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:34539
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:34539
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-gd_9gu2f
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:33275
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:40157', name: 5, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:40157
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:34395', name: 14, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:34395
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:40015', name: 10, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:40015
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:46074', name: 8, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:46074
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.181:36305', name: 4, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.181:36305
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:43525', name: 465, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:43525
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:36135', name: 466, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:36135
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:35257', name: 461, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:35257
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:34103', name: 435, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:34103
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:39231', name: 447, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:39231
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:36486', name: 428, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:36486
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:44423', name: 345, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:44423
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:36665
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:38972
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:38972
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:45166', name: 340, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:36665
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-8_kje_6v
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-tuutvjbg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:45166
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:40463', name: 352, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:33208
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:33208
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:40463
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:37800', name: 346, memory: 0, processing: 0>
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-laxxv1__
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:34808
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:34808
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:38583
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:40853
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:40853
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:38583
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:33084
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:33084
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:38329
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:38329
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:37800
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-wg7ll4h0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:40833', name: 117, memory: 0, processing: 0>
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-u99bpl94
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-lxu5tt8t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-_yj72o6z
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-25zsj1jb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:34619
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:34619
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:40833
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-g41f5djv
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:38058', name: 105, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:38058
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:36781', name: 106, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:36781
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:42035
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:43273', name: 109, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:42035
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:44801
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:44801
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-7mok6fyg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:43273
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-s9x2tc0s
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:33923', name: 278, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:33923
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:36339
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:36339
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:42340', name: 286, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-63wru7js
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:42340
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:38358
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:39225', name: 287, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:38358
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:38706
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:38706
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-o505aq29
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-x9d89f8q
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:39225
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:40869', name: 276, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:41967
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:41967
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-4r1peu20
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:40869
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:42103', name: 280, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:42103
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:37838', name: 267, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:37838
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:32869', name: 284, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:34597
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:40676
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:40676
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:34597
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:32869
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:43185', name: 281, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-xxmuso_d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-fr7a7l4p
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:37108
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:37108
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:43185
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-z9tqfuj2
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:37513', name: 274, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:37513
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:43964', name: 419, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:43964
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:43805', name: 409, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:39382
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:39382
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:43805
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:36975
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:36975
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:45177', name: 469, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ozlcynxk
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-4ykzrofu
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:45177
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:44800', name: 471, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:42788
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:42788
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-93h82aoi
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:44800
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:36484
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:36484
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:44626
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:44431
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:44431
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:44626
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-0ip7_spf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-zdggg0do
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-opq3wng4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:33963', name: 294, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:33963
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:40929', name: 301, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:42262
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:40929
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:39269
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:42262
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:39269
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:45287', name: 305, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-_way32kh
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-sr93_udb
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:45287
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:45253', name: 299, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:45253
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:43099', name: 311, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:37750
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:37750
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:39716
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:39716
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:43099
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-1kmznpz2
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:38871', name: 452, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-dilhak7r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:38871
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:42999', name: 448, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:42999
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:38603', name: 86, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:46139
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:46139
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:35850
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:35850
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:38603
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-mf51ercc
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:32973', name: 164, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-sl0uz55m
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:32973
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:46337', name: 166, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:46337
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:34314', name: 156, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:34314
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:41694', name: 147, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:41694
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:33828', name: 317, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:33828
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:43960', name: 320, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:43960
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:36695', name: 330, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:36695
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:42845
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:42845
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-2pl5xtfv
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:46628
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:46628
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-wuam0y2a
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:35772
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:34907
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:46103
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:33258
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:35772
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:34907
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:39394
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:33258
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.190:39567
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:46103
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:37485
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-03lb1apk
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.190:39567
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:39394
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:37485
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-tb0l0lj5
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-sktj7s34
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-jeacs3x6
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-l08vhfvq
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-nqwozrp7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-23nxdpxn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:43066', name: 211, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:43066
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:36958', name: 197, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:36958
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:42071', name: 394, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:42071
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:34939', name: 407, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:34939
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:44769', name: 399, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:44769
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:39785', name: 393, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:39785
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:37008', name: 387, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:37008
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:33971', name: 405, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:33971
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:35988', name: 395, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:35988
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:37401', name: 397, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:37401
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:42463', name: 391, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:42463
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:38926', name: 385, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:38926
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:42489', name: 191, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:42489
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:44294', name: 183, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:44294
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:37460', name: 188, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:37460
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:34539', name: 172, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:34539
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:46164', name: 176, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:46164
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:41984', name: 123, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:35264
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:35264
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:41984
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:38457', name: 219, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-w17b7xeo
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:36360
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:36360
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:46181
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:46181
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:41251
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:41251
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:32963
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:32963
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:38217
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:39796
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:42932
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:38217
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:39796
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:45969
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:40942
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:44377
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:42932
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:45969
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:45063
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:38457
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:44377
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:39761', name: 231, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:45063
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:43870
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-bf07k5ck
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-bfxw3klj
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:43870
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:40942
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-d7ihypa2
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:46198
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ls5rw0j7
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-9q82_4d8
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-jfr9vs3l
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-4wpoy6ic
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:45650
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:43239
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:43239
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:39761
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:45650
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:42665', name: 234, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:46198
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-g2vsqtaw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-dctnb9zx
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-a1kmslf_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-e8qw0g9z
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:37127
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-79kl1sao
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:37127
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-aatnj173
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:42665
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-0zl6vjft
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:34377', name: 229, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-s6fu1l72
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-awmr_nix
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:34377
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:46571', name: 221, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:46571
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:42028', name: 235, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:42028
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:39892', name: 225, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:39892
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:42115', name: 232, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:46693
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:46693
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:42115
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:33618
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:33618
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:34746
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:38367', name: 222, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:43939
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:43939
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:35357
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:35357
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:34746
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:38299
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:38299
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:34846
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:34846
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:45140
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:45140
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:33026
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:42321
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:46001
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:33026
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:46001
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.195:42058
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-xof4aoz1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:42321
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:38367
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-emwgr3z3
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-l6m3ld2r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-podljkmv
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:41639', name: 217, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.195:42058
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-d19xbbwp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-zqcxfejy
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-566h3_mr
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-jvzdzvli
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-74xslxzo
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-gmm556k7
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-54cm9tag
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-kl_b_0gm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:41639
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:39226', name: 228, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:43341
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:43341
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-c89v04m1
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:39226
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:44607', name: 224, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:39533
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:39533
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-b8mgef42
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:44607
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:39425', name: 366, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:37962
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:37962
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-4y8rmkxs
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:39425
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:34763
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:34763
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:40526', name: 378, memory: 0, processing: 0>
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-aftimaha
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:40526
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:42391
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:42391
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:45002', name: 374, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:38975
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:38975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-03ft69qw
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-8sleq7et
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:45002
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:40157', name: 382, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:41461
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:41461
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ssdgxxek
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.193:33995
distributed.worker - INFO -          Listening to:   tcp://172.30.0.193:33995
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-j0vm11pk
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:37533
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:37533
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-9976951u
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:44543
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:44543
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-5bez916s
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:41373
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:41373
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-lpaqzc_3
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:35261
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:35261
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-efo0zfo2
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:36009
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:36009
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-7zj4ut7m
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:40157
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:41063', name: 379, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:41063
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:44846', name: 377, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:44846
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:45750', name: 369, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:45750
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:40776', name: 373, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:40776
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:37587', name: 362, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:37587
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:43058', name: 376, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:43058
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:43514
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:43514
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-_mrmg1xl
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:46091
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:46091
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:42972
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:42972
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-vpwmouw0
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-cys396ih
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:37836
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:37836
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-49ot948j
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:34467', name: 253, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:34467
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:36886', name: 243, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:36886
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:37389', name: 245, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:37389
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:33652', name: 250, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:33652
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:42666', name: 247, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:42666
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:43673', name: 241, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:43673
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:41514', name: 263, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:41514
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:39466', name: 244, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:39466
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:38563', name: 248, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:38563
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:38917', name: 254, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:38917
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:39315', name: 249, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:39315
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:37112', name: 258, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:37112
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:39730', name: 256, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:39730
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:43398', name: 259, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:43398
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:41181', name: 240, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:41181
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:44274', name: 242, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:34084
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:34084
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:44274
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:40820', name: 252, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-os8idb_v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:40820
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:43874', name: 262, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:34045
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:34045
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3qbgkq5x
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:43874
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:34200', name: 255, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:35156
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:35156
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-g464gi7y
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:34200
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:40848', name: 257, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:40848
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:35418', name: 251, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:36899
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:39994
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:36899
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:35418
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:39994
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:43684', name: 49, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-376kqcli
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-_sveq033
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:43684
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:33407', name: 246, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:33407
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:38640', name: 261, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:38640
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.191:45158', name: 260, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.191:45158
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:36665', name: 338, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:36665
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:38972', name: 353, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:38972
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:33208', name: 349, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:33208
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:34808', name: 371, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:34808
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:38583', name: 367, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:38583
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:37960
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:37960
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:40853', name: 372, memory: 0, processing: 0>
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-92fyhzyo
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:37882
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:37882
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ibqwh1_o
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:40853
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:33084', name: 227, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:33084
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:38329', name: 237, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:38329
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:34619', name: 236, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:34619
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:42035', name: 226, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:42035
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:44801', name: 216, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:44801
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:36339', name: 239, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:36339
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:38358', name: 223, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:38358
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:38706', name: 230, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:38706
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:41967', name: 218, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:41967
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:34597', name: 178, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:34597
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:40676', name: 189, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:40676
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:37108', name: 233, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:37108
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:36975', name: 170, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:36975
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:39382', name: 182, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:39382
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:42788', name: 186, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:42788
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:36484', name: 184, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:36484
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:44626', name: 169, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:44626
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:44431', name: 187, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:44431
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:42262', name: 173, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:42262
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:39269', name: 185, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:46388
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:46388
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:39269
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:39716', name: 116, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-7xrqe8j8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:34222
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:39716
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:34222
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:37750', name: 112, memory: 0, processing: 0>
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:41900
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:41900
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-zo9_0d64
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-2z10v4qe
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:37750
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:35850', name: 103, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:35850
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:46139', name: 98, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:41162
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:41162
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-98bqkdgb
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:46367
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:46367
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-h2i_g6de
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:46139
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:42845', name: 356, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:42845
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:46628', name: 344, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:34320
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:41471
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:46628
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:34320
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:41471
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:34907', name: 357, memory: 0, processing: 0>
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-bti7r0qc
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-dzsmj506
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:34907
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:33258', name: 168, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:33258
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:35772', name: 238, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:35772
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:37485', name: 174, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:37485
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.190:39567', name: 220, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.190:39567
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:46103', name: 329, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:46103
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:39394', name: 328, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:39394
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:38143
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:38143
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:42744
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:42744
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:35373
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:35315
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:35373
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:35315
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-9odf_voc
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-6p8ewc_x
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-1yyuo8gz
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-e7takbep
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:36377
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:43734
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:38870
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:36377
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:43993
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:43734
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:38870
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:43993
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:37291
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:37291
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:34428
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:34428
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-70q7puce
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-5efwciam
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-eih4zc59
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-cc3_n6r1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:38863
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-knfw2yze
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:38863
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-4jec5tuo
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-8v3pcmuo
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.188:36437
distributed.worker - INFO -          Listening to:   tcp://172.30.0.188:36437
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-1rf7rqtn
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:35264', name: 113, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:35264
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:36360', name: 359, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:36360
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:46181', name: 348, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:46181
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:41251', name: 342, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:41251
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:45063', name: 351, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:45063
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:38217', name: 339, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:38217
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:32963', name: 302, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:32963
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:45969', name: 354, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:45969
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:45650', name: 343, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:45650
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:42932', name: 306, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:42932
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:39796', name: 304, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:39648
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:39648
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-pyf7pf_d
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:39796
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:46198', name: 300, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:46198
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:44377', name: 298, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:44377
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:40942', name: 292, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:40942
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:43239', name: 296, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:45900
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:45900
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-4ig0zcmv
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:43239
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:43870', name: 308, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:45265
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:45265
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.196:40975
distributed.worker - INFO -          Listening to:   tcp://172.30.0.196:40975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:38860
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:43870
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:38860
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:37127', name: 358, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3fcsotzx
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3idokyfn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-cqyu1iqo
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:37127
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:45936
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:45936
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:46693', name: 290, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-hszh3wyc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:46693
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:43939', name: 347, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:35906
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:35906
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-_h05ouj0
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:43939
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:34898
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:34898
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:33618', name: 297, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-nwz5wpty
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:33618
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:34746', name: 337, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:34746
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:38299', name: 291, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:41503
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:41503
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-xh3pi99j
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:38299
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:35357', name: 288, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:35357
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:34846', name: 355, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:34846
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:45140', name: 310, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:45140
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:42058', name: 341, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:39536
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:42213
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:42213
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:42058
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:39536
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:33033
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:33033
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:42321', name: 336, memory: 0, processing: 0>
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-31kog0m4
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-sd8sh733
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-vs2bmu78
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:42321
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:46001', name: 307, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:46001
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.195:33026', name: 350, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.195:33026
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:43341', name: 289, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:34014
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:34014
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-l1fjidxv
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:43341
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:39533', name: 293, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:39533
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:37962', name: 309, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:37962
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:34763', name: 295, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:34763
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:38975', name: 181, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:38975
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:42391', name: 175, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:42391
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:41461', name: 190, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:41461
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.193:33995', name: 303, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.193:33995
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:37533', name: 375, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:37533
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:44543', name: 365, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:44543
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:41373', name: 380, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:41373
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:35261', name: 370, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:35261
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:36009', name: 364, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:36009
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:43514', name: 73, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:46467
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:46467
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:43514
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-8hpohaf8
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:46091', name: 94, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:36189
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:41541
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:36189
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:41541
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:37948
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:37948
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-hw1yxvch
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-piuvw6ol
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-8gvf681_
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:46091
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:42972', name: 76, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:42972
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:37836', name: 383, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:33901
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:33901
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-nkod0g8d
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:37836
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:34084', name: 368, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:40669
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:40642
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:40642
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:40669
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-m_fhre_7
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-95nwhgpy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:39199
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:39199
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:34084
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-iq9ybjya
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:34045', name: 396, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:34045
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:35156', name: 402, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:35156
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:39994', name: 401, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:39994
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:36974
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:45000
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:36899', name: 404, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:36974
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:36337
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:45000
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:36337
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3_ozsluv
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-31f81niv
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-qi57ozai
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:36899
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:37960', name: 285, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:37960
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:37882', name: 275, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:41732
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:41732
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-akqu5toa
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:34042
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:34042
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-64mebelt
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:37882
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:46388', name: 67, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:45797
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:45797
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-8eom0ldc
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:46388
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:34222', name: 61, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:34222
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:41900', name: 52, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:44112
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:44112
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-6rpy70d7
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:41900
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:41162', name: 59, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:41162
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:46367', name: 57, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:40879
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:40879
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-xatoaprd
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:46367
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:34320', name: 55, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:34320
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:41471', name: 71, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:36530
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:36530
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:36698
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:36698
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-6_z_n_xm
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-r9xm2k9m
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:33044
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:41471
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:33044
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:33995
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:38143', name: 215, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:33995
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-bqle11c_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.185:33748
distributed.worker - INFO -          Listening to:   tcp://172.30.0.185:33748
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-z_9ghojt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ihu6pdah
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:38143
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:42744', name: 196, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:37227
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:37227
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-k7ir2wcj
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:42744
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:35373', name: 206, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:35373
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:35315', name: 208, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:35315
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:33311
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:33311
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-yixc5sf9
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:37299
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:37299
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:33518
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:33518
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:33503
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:33503
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:33800
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-m7ilesre
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:33800
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-5vfour9t
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:43759
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:43759
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-9to3exef
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3ln59e78
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-demmoa0g
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:33298
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:33298
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:43992
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:43992
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-i961x06q
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-yeo9tcbf
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:37631
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:37631
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-1r04omcu
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:37029
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:37029
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-57o5ld8k
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:37332
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:37332
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-pxwm6eoq
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.129:38155
distributed.worker - INFO -          Listening to:   tcp://172.30.5.129:38155
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:43734', name: 207, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-tselfrmc
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:43734
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:36377', name: 199, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:39571
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:39571
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-1w_xk3ae
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:36466
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:36466
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:36377
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-gr0c75xs
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:38870', name: 360, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:38870
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:43993', name: 363, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:46091
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:46091
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:43638
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:43638
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:43993
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:34428', name: 180, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:44138
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ohf9jr2m
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:44138
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-34sspgmu
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-vaboqgu7
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:41168
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:41168
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:34428
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:37291', name: 177, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-dprmo6m2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:37291
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:38863', name: 179, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:44702
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:40003
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:44702
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:40003
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:45733
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:45733
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-gxj39l3j
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-h3zwfncb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-u8mdtmjz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:38863
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.188:36437', name: 171, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:38094
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:38094
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:42760
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:42760
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.188:36437
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-hvp_bzrd
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-7gh5pd9f
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:39648', name: 381, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:39648
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:45900', name: 413, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:44170
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:44170
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-rpqe6yk8
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:40616
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:40616
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-7uynayv5
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:45900
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:45265', name: 426, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:45265
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.196:40975', name: 361, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.196:40975
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:38860', name: 414, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:33408
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:33408
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:39019
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:39019
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-l43ylr0x
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-jfyf3uug
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:46384
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:46384
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-9ys_kx_g
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:38751
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:38751
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-k5wn1pk3
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:38860
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:45936', name: 425, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.192:46346
distributed.worker - INFO -          Listening to:   tcp://172.30.0.192:46346
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-uo1cs8vl
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:45936
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:35906', name: 427, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:35906
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:34898', name: 411, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:34898
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:41503', name: 429, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:41503
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:39536', name: 449, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:39536
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:45745
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:33033', name: 453, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:45745
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:40187
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:40187
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-20npysk_
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:39168
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:39168
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-mt_s4qw3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-rle4pc4n
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:33033
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:44832
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:42213', name: 442, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:44832
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:35145
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:35145
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-i3ldq_ie
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-vnucqhys
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:35025
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:35025
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:42213
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ayc40dxi
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:34014', name: 432, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:33711
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:33711
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-nn1ebbg_
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:34014
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:46467', name: 97, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:42356
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:42356
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:46457
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:46457
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-p8kh__cv
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:46467
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-4rlo3ntf
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:41541', name: 119, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:43784
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:43784
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-pnoegi1g
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:39922
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:39922
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:45292
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:45292
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-btmnwl46
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:41541
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-g2jhp6z3
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:36189', name: 118, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:36748
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:45642
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:45642
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:36748
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:36189
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:37948', name: 96, memory: 0, processing: 0>
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ma7hsy79
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-be_pn2fv
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:43774
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:43774
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-kq6nh6x_
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:37948
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:33901', name: 101, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:33901
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:40669', name: 115, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:40669
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:40642', name: 99, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:40642
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:39199', name: 107, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:39199
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:36974', name: 100, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:34262
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:34262
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-d11s4vmc
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:36974
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:45000', name: 114, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:45000
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:36337', name: 403, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:36337
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:41732', name: 398, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:34217
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:34217
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:41133
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:41133
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-c_pvm6ok
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:41732
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:34042', name: 110, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:44852
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:44852
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:45717
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-1o5gdd2g
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:45717
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:38848
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:38848
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:41018
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:41018
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3ee8wv_l
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-bbcrzep5
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:34042
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-29e0a69d
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:45797', name: 104, memory: 0, processing: 0>
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-rln0bhf0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.189:36162
distributed.worker - INFO -          Listening to:   tcp://172.30.0.189:36162
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-nt2rbsq9
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:45797
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:45212
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:44112', name: 102, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:45212
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-356vacai
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:43051
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:43051
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-vvh3f35j
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:44180
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:44180
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:44112
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-nn3sun1w
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:40879', name: 389, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:40879
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:36698', name: 406, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:34098
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:41585
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:34098
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:41585
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-gsdtijek
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-u9lknjkf
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:36698
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:36530', name: 390, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:37335
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:37335
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-59a0uyx1
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:36530
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:33044', name: 400, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:33044
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:33995', name: 111, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:33995
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.185:33748', name: 108, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.185:33748
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:37227', name: 392, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:37227
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:32977
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:32977
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-607rhic1
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:36992
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:36992
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-shpjozxq
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:43373
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:43373
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-sz9y_iin
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:42886
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:42886
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-g6b9n33y
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:44935
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:44935
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:38244
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:38244
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ksfk7rnm
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-j7ltw7zq
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:36056
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:36056
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3e_efusa
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:40140
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:40140
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-4zo2mq0t
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:42401
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:42401
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:32857
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:32857
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:33311', name: 77, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:35711
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:35711
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-4628u5f0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-zliq33l8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-x5hnsmtm
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:33311
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:37299', name: 83, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:37299
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:33518', name: 90, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.183:35690
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:35318
distributed.worker - INFO -          Listening to:   tcp://172.30.0.183:35690
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:35318
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-vui3oiay
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-14985f8v
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:45915
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:45915
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-lvxggd51
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:33518
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:33503', name: 82, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:33503
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:33800', name: 88, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:42219
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:42219
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:33800
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:38846
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:33298', name: 92, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:38846
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-l3qiu6d3
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-eix_fe_9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:33298
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:43759', name: 386, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:41968
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:41968
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:42318
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:42318
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-8xi62tjs
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:43759
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-e08oyzum
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:43992', name: 384, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:46102
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:46102
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-xsunt5nn
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:40011
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:40011
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-njss5mj1
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:43992
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:40978
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:40978
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:37631', name: 75, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-7ktpkop8
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:37631
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:37029', name: 270, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:37029
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:37332', name: 277, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:37332
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:33089
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:33089
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.129:38155', name: 388, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-gis7zfcn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:43489
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:43489
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.129:38155
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-038cotlb
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:39571', name: 271, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:39571
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:36466', name: 265, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:39437
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:39437
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-19nzyzb_
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:37512
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:37512
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:42823
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:42823
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:39948
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:36466
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:39948
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-usdebez4
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:46091', name: 269, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-aafg9vjy
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-fa4gk_h5
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:46091
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:43638', name: 283, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:36947
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:36947
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-9yb_q7f0
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:43638
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:44179
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:44138', name: 194, memory: 0, processing: 0>
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:44179
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-53_rpccz
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:33167
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:33167
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.184:42729
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.184:42729
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-xwq1ar1z
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:44138
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:41168', name: 279, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-poty313l
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:41168
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:44702', name: 210, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:46563
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:46563
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:34045
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:34045
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:44702
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-qkby9nim
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-t4i34ug2
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:40003', name: 198, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:40003
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:45733', name: 268, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:33283
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:33283
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:35376
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:35376
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-1lu2h44k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-37mt7htf
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:45733
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:38094', name: 214, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:44992
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:38094
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:44992
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:42760', name: 212, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-modl4jmz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:42760
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:44170', name: 209, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:45383
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:45383
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-te_yk8fj
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:44170
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:35059
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:34381
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:35059
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:34381
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:36522
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:40616', name: 202, memory: 0, processing: 0>
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:36522
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-d0v1p130
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-zpze_goj
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ofbwh0ys
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:40616
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:33408', name: 282, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:43614
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:43614
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-m4vibj4k
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:33408
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:39019', name: 273, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:39019
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:41329
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:41329
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:46384', name: 272, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-o0z6i6c6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:46613
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:46613
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:42595
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:42595
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-p7mfw5xc
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-o4i5vzmq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:46384
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:38751', name: 264, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:34167
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:34167
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-yijcjuaq
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:38751
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.192:46346', name: 266, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:45815
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:45815
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-4ewa0naz
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.187:33975
distributed.worker - INFO -          Listening to:   tcp://172.30.0.187:33975
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.192:46346
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-wwbagwu7
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:45745', name: 68, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:45745
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:41654
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:41654
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:40187', name: 69, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-9_zhokln
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:40187
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:39168', name: 65, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:33914
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:33914
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-n5lc8o6e
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:39168
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:44832', name: 193, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:42314
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:42314
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-29k5_bv6
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:40025
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:44832
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:40025
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:35145', name: 62, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-e2vsklfs
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:35145
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:35025', name: 54, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:38067
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:38067
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-bzulxf2z
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:35025
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:33711', name: 66, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:44324
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:44324
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-rlfj51tc
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:33711
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:42356', name: 205, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:36612
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:35159
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:36612
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:35159
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:42356
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:46457', name: 213, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-cefo74qk
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-pgdg8xvf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:43085
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:43085
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:46457
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.136:37056
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.136:37056
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:43784', name: 63, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3k6da18x
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-389oeo9o
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:43784
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:39922', name: 203, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:39922
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:45292', name: 204, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:45292
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:45642', name: 200, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:45642
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:36748', name: 195, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:36748
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:43774', name: 201, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:43774
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:34262', name: 424, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:34262
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:34217', name: 417, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:34217
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:41133', name: 415, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:41133
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:45717', name: 421, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:45717
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:44852', name: 422, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:44852
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:38848', name: 410, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:38848
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:41018', name: 70, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:46655
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:46655
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:37246
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:37246
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:46288
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:46288
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:36461
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:36461
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:46622
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:46622
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:42995
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:42995
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:35569
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:35569
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-pj0mu2tx
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-si30a_7a
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-g0f316od
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-bha94qln
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-sqgd86mz
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-_7zxfqlx
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:40934
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:40934
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-2edepp99
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-exd6jeyj
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:41620
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:41620
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:39977
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:39977
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-297k5eyx
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-orqiupsi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:38715
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:38715
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:39735
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:39735
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-bz40bid8
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-x_zohqf4
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:40531
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:40531
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:36571
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:36571
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-fo8l_31s
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-h56b4ttd
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:46112
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:46112
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ixvfib04
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:39593
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:39593
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-8lb1tcg6
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:41018
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:35839
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:35839
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.189:36162', name: 192, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-w6tv1hrs
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:46229
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:46229
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-pq68kiut
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.189:36162
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:45212', name: 412, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:45157
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:45157
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:45212
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:43051', name: 60, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:43437
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:43437
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-mcgtc2v5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-b2wj36xa
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:43051
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:44180', name: 430, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:40096
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:40096
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-mr6fwwu0
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:34037
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:34037
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:44180
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:34098', name: 50, memory: 0, processing: 0>
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3d4e0247
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:34098
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:41585', name: 48, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:41585
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:37335', name: 64, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:37335
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:40336
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.182:43037
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:40336
distributed.worker - INFO -          Listening to:   tcp://172.30.0.182:43037
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-4r7argug
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-uon5rhul
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:32977', name: 58, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:35244
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:35244
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-uiwc8l6z
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:32977
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:36992', name: 165, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:40242
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:40242
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:41206
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:36992
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:41206
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:43373', name: 148, memory: 0, processing: 0>
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-mk2ykadn
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-masf0293
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:45979
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:45979
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:38563
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:38563
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-mdqp7wg5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-y1g53bql
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:43373
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:42886', name: 146, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:36750
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:34047
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:36750
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:34047
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:40798
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:40798
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-t3lx9_zr
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-hzqd5hun
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-nwwhivud
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:42886
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:38244', name: 53, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:38244
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:44935', name: 51, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:34131
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:34131
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-eeecmpl6
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:44935
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:36056', name: 162, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:36056
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:46257
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:46257
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-_93op5c0
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:41271
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:37191
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:37191
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:41271
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-i51kjhhg
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-pyja9940
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:41303
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:41303
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-t9q20epi
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:43570
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:43570
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-o3h29nn6
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:42806
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:42806
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-uuoryive
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:33986
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:33986
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-xtjm8z3z
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:42424
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:42424
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:38164
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:38164
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:33052
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-xuxj5zmc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:33052
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ph4b_kg3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-g3v5uylq
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:39043
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:39043
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-cm3v1t_4
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.194:33130
distributed.worker - INFO -          Listening to:   tcp://172.30.0.194:33130
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-1g6psa4m
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:40140', name: 431, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:40140
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:32857', name: 158, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:32857
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:42401', name: 150, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:39514
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:39514
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:33063
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:33063
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:36801
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:36801
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:45992
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:45992
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:33469
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:33469
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:42401
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:35711', name: 160, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-xn230sll
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:36038
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-7afkvexk
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:36038
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-toltoka5
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:42085
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-a_glt_og
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-bl6xc7c9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:42085
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-i09_d__j
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-c0ncpqzi
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:35711
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:35318', name: 89, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:39232
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:40957
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:40957
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:39232
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:36531
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-zv1utfdy
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:36531
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-_rvrbgfj
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-6a2vkn9k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:35050
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:35050
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:35318
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-by7q86xe
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.183:35690', name: 56, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:37638
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:37638
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-j_olvh4v
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:38304
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:38304
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-e1_2g25s
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.183:35690
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.131:37640
distributed.worker - INFO -          Listening to:   tcp://172.30.5.131:37640
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:45915', name: 74, memory: 0, processing: 0>
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-4l1oxb4h
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:45915
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:42219', name: 78, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:42219
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:38846', name: 93, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:43753
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:43753
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-zsamt27i
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:38187
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:38846
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:38187
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:41968', name: 81, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-mfwhmq65
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:46259
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:46259
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:34709
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:34709
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-gushdy6a
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:41968
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-ijx6lhl6
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:42318', name: 84, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.5.130:40542
distributed.worker - INFO -          Listening to:   tcp://172.30.5.130:40542
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-1hq8i61h
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:42318
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:46102', name: 91, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:46102
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:40011', name: 144, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:40011
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:40978', name: 87, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:40978
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:33089', name: 155, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:33089
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:43489', name: 470, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:43489
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:42823', name: 72, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:42823
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:39437', name: 79, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:39437
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:37512', name: 161, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:37512
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:39948', name: 468, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:39948
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:36947', name: 95, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:36947
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:44179', name: 145, memory: 0, processing: 0>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:43277
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:43277
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-gojuerq8
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:44179
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:33167', name: 85, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:33167
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.184:42729', name: 80, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.184:42729
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:34045', name: 163, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:34045
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:46563', name: 151, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:46563
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:33283', name: 153, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:33283
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:35376', name: 167, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:46075
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:46075
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-rm9dtyc0
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:36217
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:36217
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-wun_br46
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:35376
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:44992', name: 149, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:46397
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:46397
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-vna6ir6z
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:44992
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:45383', name: 460, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:45383
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:36522', name: 456, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:45942
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:45942
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:42019
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:42019
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:42187
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:42187
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-lqo6q6pn
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-v5h9croa
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-3ei23zb8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:39120
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:39120
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:36522
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-n3_y6k8b
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:34381', name: 152, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:41112
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:41112
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-y_aegocp
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:34381
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:35059', name: 462, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:36673
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:36673
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-lsi_izp1
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:35059
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:43614', name: 472, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:43614
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:41329', name: 157, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:45467
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:45467
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-buep7rxx
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:41329
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:42595', name: 464, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:34103
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:37397
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:37397
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:34103
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:42595
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:46613', name: 474, memory: 0, processing: 0>
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-113mahn0
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-pfv0c8zt
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:37933
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:37933
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-nivj7yy1
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:34234
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:34234
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:39747
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-hcs60dr_
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:39747
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:46613
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-d7sp5pzq
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:34167', name: 159, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:34167
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:45815', name: 459, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:37603
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:37603
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-q3p8xty_
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:45815
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.187:33975', name: 154, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:44714
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:44714
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-84o7z5g8
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.187:33975
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:41654', name: 478, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:38987
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:34267
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:38987
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:34267
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-87ovqqrl
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-bdi48zf_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:37509
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:37509
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:37914
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:37914
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-eezsjluv
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:41654
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-anm6hfst
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:33914', name: 476, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py:487: UserWarning: The local_dir keyword has moved to local_directory
  warnings.warn("The local_dir keyword has moved to local_directory")
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.0.186:41761
distributed.worker - INFO -          Listening to:   tcp://172.30.0.186:41761
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   30.93 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/DEV/git/eNATL60-plots-paper/fine-scale-vorticity-variance/worker-7jr0luxz
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:33914
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:42314', name: 477, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:42314
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:40025', name: 458, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:40025
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:38067', name: 467, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:38067
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:44324', name: 473, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:44324
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:36612', name: 463, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:36612
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:35159', name: 475, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:35159
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:43085', name: 457, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:43085
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.136:37056', name: 479, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.136:37056
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:35569', name: 44, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:35569
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:40934', name: 47, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:40934
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:37246', name: 33, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:37246
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:46622', name: 39, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:46622
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:46655', name: 31, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:46655
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:42995', name: 43, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:42995
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:46288', name: 34, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:46288
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:39977', name: 41, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:39977
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:36461', name: 35, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:36461
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:41620', name: 24, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:41620
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:38715', name: 45, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:38715
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:39735', name: 36, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:39735
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:40531', name: 38, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:40531
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:36571', name: 28, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:36571
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:46112', name: 37, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:46112
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:39593', name: 27, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:39593
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:35839', name: 29, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:35839
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:46229', name: 42, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:46229
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:45157', name: 25, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:45157
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:43437', name: 26, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:43437
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:40096', name: 46, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:40096
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:34037', name: 30, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:34037
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:40336', name: 40, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:40336
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.182:43037', name: 32, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.182:43037
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:35244', name: 332, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:35244
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:40242', name: 331, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:40242
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:41206', name: 316, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:41206
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:45979', name: 326, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:45979
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:38563', name: 315, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:38563
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:36750', name: 323, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:36750
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:40798', name: 312, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:40798
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:34047', name: 324, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:34047
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:34131', name: 334, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:34131
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:46257', name: 319, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:46257
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:37191', name: 327, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:37191
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:41271', name: 313, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:41271
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:41303', name: 325, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:41303
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:43570', name: 441, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:43570
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:42806', name: 318, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:42806
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:33986', name: 314, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:33986
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:42424', name: 322, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:42424
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:38164', name: 321, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:38164
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:33052', name: 333, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:33052
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:39043', name: 433, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:39043
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.194:33130', name: 335, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.194:33130
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:45992', name: 443, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:45992
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:39514', name: 437, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:39514
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:33063', name: 451, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:33063
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:36801', name: 439, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:36801
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:33469', name: 445, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:33469
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:36038', name: 455, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:36038
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:42085', name: 444, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:42085
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:39232', name: 438, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:39232
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:40957', name: 450, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:40957
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:36531', name: 436, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:36531
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:35050', name: 434, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:35050
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:37638', name: 446, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:37638
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:38304', name: 440, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:38304
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.131:37640', name: 454, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.131:37640
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:43753', name: 423, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:43753
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:38187', name: 420, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:38187
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:46259', name: 418, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:46259
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:34709', name: 416, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:34709
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.5.130:40542', name: 408, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.5.130:40542
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:43277', name: 139, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:43277
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:46075', name: 120, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:46075
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:36217', name: 134, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:36217
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:46397', name: 126, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:46397
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:42019', name: 138, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:42019
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:42187', name: 140, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:42187
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:45942', name: 132, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:45942
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:39120', name: 133, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:39120
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:41112', name: 127, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:41112
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:36673', name: 128, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:36673
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:45467', name: 125, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:45467
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:37397', name: 137, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:37397
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:34103', name: 121, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:34103
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:37933', name: 122, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:37933
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:34234', name: 135, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:34234
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:39747', name: 131, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:39747
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:37603', name: 136, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:37603
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:44714', name: 142, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:44714
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:38987', name: 141, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:38987
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:34267', name: 143, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:34267
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:37509', name: 124, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:37509
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:37914', name: 129, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:37914
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://172.30.0.186:41761', name: 130, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.30.0.186:41761
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.0.181:42447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-6fffd48c-861c-11ea-81b6-0800383bad01
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Run out-of-band function 'stop'
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:36411
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:44192
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:42279
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:37847
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:36016
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:45563
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:42130
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:42985
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:35828
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:43637
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:38820
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:45738
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:36363
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:37865
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:45238
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:37634
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:33275
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:40157
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:34395
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:40015
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:36305
distributed.worker - INFO - Stopping worker at tcp://172.30.0.181:46074
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:35257
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:39231
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:43525
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:34103
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:36486
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:37800
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:36135
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:43805
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:40463
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:38058
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:43964
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:44423
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:40833
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:45166
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:45177
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:43273
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:44800
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:38871
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:36781
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:42999
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:40929
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:45253
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:33963
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:34314
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:45287
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:43099
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:32973
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:37838
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:41694
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:38603
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:40869
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:43960
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:46337
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:43185
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:36695
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:39225
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:33828
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:43066
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:42103
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:44769
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:36958
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:33923
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:37513
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:35988
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:32869
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:37008
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:42340
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:42463
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:34939
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:37460
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:42071
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:44294
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:39785
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:42489
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:33971
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:41984
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:46164
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:38926
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:34539
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:37401
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:39892
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:42028
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:39761
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:46571
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:42665
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:34377
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:38367
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:42115
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:41639
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:45750
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:39226
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:39425
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:40157
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:44607
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:43058
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:40776
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:41063
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:44846
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:40526
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:45002
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:41514
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:36665
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:37587
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:43673
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:38972
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:33208
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:34467
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:34808
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:33652
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:43684
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:36886
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:40853
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:39466
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:34619
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:38583
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:38917
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:33084
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:39315
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:38457
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:37389
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:38563
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:39730
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:38329
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:34597
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:42035
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:40676
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:37112
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:36975
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:44801
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:36339
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:39716
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:38358
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:37750
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:43398
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:42666
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:42788
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:46628
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:38706
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:34907
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:46139
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:41181
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:41967
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:42845
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:35850
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:40820
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:36484
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:44274
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:37108
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:35264
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:46181
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:36360
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:46103
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:39382
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:35772
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:39394
distributed.worker - INFO - Stopping worker at tcp://172.30.0.190:39567
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:32963
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:44626
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:38217
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:44431
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:41251
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:42262
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:40848
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:39269
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:35418
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:42932
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:45063
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:34200
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:38640
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:45650
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:33258
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:45158
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:37485
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:45969
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:33407
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:39796
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.191:43874
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:46198
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:37127
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:43939
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:44377
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:40942
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:34746
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:43239
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:34846
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:42058
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:38975
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:46693
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:42321
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:42391
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:43870
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:33618
distributed.worker - INFO - Stopping worker at tcp://172.30.0.195:33026
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:41461
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:37533
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:46091
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:42972
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:34045
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:38299
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:44543
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:35156
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:35357
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:35261
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:45140
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:41373
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:37960
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:43514
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:36009
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:36899
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:37882
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:39994
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:37836
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:39533
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:34084
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:46388
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:46001
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:42744
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:37962
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:38870
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:38143
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:34222
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:34428
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:34763
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:41162
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:43993
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:35373
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:41900
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:43341
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:46367
distributed.worker - INFO - Stopping worker at tcp://172.30.0.193:33995
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:35315
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:37291
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:34320
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:38863
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:39648
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:36377
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:41471
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.188:36437
distributed.worker - INFO - Stopping worker at tcp://172.30.0.196:40975
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:43734
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:45900
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:45265
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:45936
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:35906
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:39536
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:42213
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:46467
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:34898
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:34014
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:41503
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:33033
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:38860
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:36189
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:41541
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:41732
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:36337
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:37948
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:33901
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:40669
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:40642
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:36530
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:37299
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:39199
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:40879
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:36974
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:33311
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:45000
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:36698
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:33044
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:33518
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:33503
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:34042
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:37227
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:45797
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:37029
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:33800
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:44112
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:43759
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:37332
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:33298
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:44138
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:37631
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:33748
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:39571
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:43992
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.185:33995
distributed.worker - INFO - Stopping worker at tcp://172.30.5.129:38155
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:44702
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:36466
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:40003
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:46091
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:45745
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:43638
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:41168
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:42760
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:45733
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:38094
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:40187
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:39019
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:33408
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:44170
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:35145
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:40616
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:35025
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:46384
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:44832
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:33711
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:34262
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:38751
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:39168
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:34217
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.192:46346
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:42356
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:43784
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:41018
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:46457
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:38848
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:41133
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:39922
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:45717
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:43051
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:45292
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:45642
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:44852
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:45212
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:36748
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:34098
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:43774
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:36992
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:44180
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:41585
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:35318
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.189:36162
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:37335
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:42886
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:40140
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:32977
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:36056
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:38244
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:38846
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:44935
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:43373
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:43489
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:42219
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:32857
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.183:35690
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:45915
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:39948
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:41968
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:35711
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:42401
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:46102
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:42318
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:33089
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:40978
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:40011
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:39437
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:45383
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:36522
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:42823
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:37512
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:36947
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:33167
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:35059
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.184:42729
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:43614
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:44179
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:34045
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:42595
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:46563
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:33283
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:46613
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:44992
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:35376
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:41329
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:41654
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:34381
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:40025
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:45815
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:34167
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:41620
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.187:33975
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:42995
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:36461
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:37246
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:35244
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:36571
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:44324
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:39735
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:40242
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:46112
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:41206
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:46655
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:45979
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:35569
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:46622
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:40531
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:38067
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:40934
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:43570
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:36750
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:42314
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:38715
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:35159
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:46288
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:33914
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:39043
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:39977
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:34047
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:43085
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:35839
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:38563
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:36612
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:39593
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:39514
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.136:37056
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:45992
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:46257
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:33063
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:34131
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:46229
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:37191
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:45157
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:43437
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:40798
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:41303
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:42085
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:41271
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:40096
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:43753
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:36801
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:36038
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:34037
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:33469
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:42806
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:38187
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:40336
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:36217
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:39232
distributed.worker - INFO - Stopping worker at tcp://172.30.0.182:43037
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:46259
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:40957
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:42424
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:34709
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.5.130:40542
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:46075
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:36531
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:33986
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:43277
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:37638
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:38164
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:35050
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:46397
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:38304
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:33052
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.194:33130
distributed.worker - INFO - Stopping worker at tcp://172.30.5.131:37640
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:42019
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:42187
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:45942
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:39120
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:36673
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:41112
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:45467
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:37397
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:34103
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:34234
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:39747
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:37933
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:37603
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:44714
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:38987
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:37509
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:41761
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:37914
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Stopping worker at tcp://172.30.0.186:34267
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:36411', name: 12, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:36411
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:44192', name: 15, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:44192
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:42279', name: 2, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:42279
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:37847', name: 7, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:37847
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:36016', name: 6, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:36016
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:45563', name: 21, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:45563
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:42130', name: 3, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:42130
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:42985', name: 18, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:42985
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:35828', name: 23, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:35828
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:43637', name: 13, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.181:43637
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:38820', name: 16, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:38820
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:36363', name: 17, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:36363
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:45738', name: 11, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:45738
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:37865', name: 19, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.181:37865
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:37634', name: 22, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:37634
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:45238', name: 9, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.181:45238
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:33275', name: 20, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:33275
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:40157', name: 5, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:40157
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:34395', name: 14, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:34395
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:40015', name: 10, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:40015
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:46074', name: 8, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:46074
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.181:36305', name: 4, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.181:36305
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:35257', name: 461, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:35257
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:43525', name: 465, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:43525
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:36135', name: 466, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:36135
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:39231', name: 447, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:39231
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:34103', name: 435, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:34103
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:36486', name: 428, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:36486
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:37800', name: 346, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:37800
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:40463', name: 352, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:40463
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:44423', name: 345, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:44423
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:45166', name: 340, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:45166
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:36781', name: 106, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:36781
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:40833', name: 117, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:40833
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:43273', name: 109, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:43273
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:38058', name: 105, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:38058
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:43964', name: 419, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:43964
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:43805', name: 409, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:43805
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:45177', name: 469, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:45177
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:44800', name: 471, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:44800
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:38871', name: 452, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:38871
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:42999', name: 448, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:42999
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:43185', name: 281, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:43185
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:40869', name: 276, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:40869
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:32869', name: 284, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:32869
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:37838', name: 267, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.192:37838
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:37513', name: 274, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:37513
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:42103', name: 280, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:42103
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:39225', name: 287, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.192:39225
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:42340', name: 286, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:42340
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:33923', name: 278, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:33923
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:43099', name: 311, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:43099
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:45287', name: 305, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:45287
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:33963', name: 294, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:33963
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:45253', name: 299, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:45253
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:40929', name: 301, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:40929
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:34314', name: 156, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:34314
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:41694', name: 147, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:41694
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:32973', name: 164, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:32973
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:46337', name: 166, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:46337
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:36695', name: 330, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:36695
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:43960', name: 320, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:43960
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:33828', name: 317, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:33828
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:43066', name: 211, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:43066
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:36958', name: 197, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:36958
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:38603', name: 86, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:38603
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:42071', name: 394, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.5.129:42071
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:35988', name: 395, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:35988
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:44769', name: 399, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:44769
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:33971', name: 405, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:33971
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:37401', name: 397, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:37401
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:42463', name: 391, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:42463
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:39785', name: 393, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:39785
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:37008', name: 387, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:37008
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:34939', name: 407, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:34939
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:38926', name: 385, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:38926
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:37460', name: 188, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:37460
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:34539', name: 172, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:34539
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:46164', name: 176, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:46164
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:44294', name: 183, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:44294
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:42489', name: 191, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:42489
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:41984', name: 123, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:41984
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:46571', name: 221, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:46571
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:39892', name: 225, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:39892
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:42115', name: 232, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:42115
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:34377', name: 229, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:34377
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:42665', name: 234, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:42665
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:42028', name: 235, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:42028
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:38367', name: 222, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:38367
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:39761', name: 231, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:39761
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:41639', name: 217, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:41639
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:39226', name: 228, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:39226
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:44607', name: 224, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:44607
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:40526', name: 378, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:40526
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:43058', name: 376, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:43058
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:45750', name: 369, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:45750
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:37587', name: 362, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:37587
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:39425', name: 366, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:39425
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:40157', name: 382, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:40157
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:44846', name: 377, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:44846
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:40776', name: 373, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:40776
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:45002', name: 374, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:45002
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:41063', name: 379, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:41063
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:33652', name: 250, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:33652
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:36886', name: 243, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:36886
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:37389', name: 245, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:37389
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:41514', name: 263, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:41514
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:43673', name: 241, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:43673
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:42666', name: 247, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:42666
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:39466', name: 244, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.191:39466
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:34467', name: 253, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:34467
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:38917', name: 254, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:38917
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:38563', name: 248, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:38563
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:39315', name: 249, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:39315
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:37112', name: 258, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:37112
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:39730', name: 256, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:39730
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:43398', name: 259, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:43398
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:41181', name: 240, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:41181
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:44274', name: 242, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:44274
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:40820', name: 252, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.191:40820
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:34200', name: 255, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.191:34200
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:40848', name: 257, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:40848
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:35418', name: 251, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.191:35418
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:33407', name: 246, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:33407
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:38640', name: 261, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:38640
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:45158', name: 260, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.191:45158
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:36665', name: 338, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:36665
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:38972', name: 353, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.195:38972
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.191:43874', name: 262, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.191:43874
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:33208', name: 349, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.195:33208
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:34808', name: 371, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:34808
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:43684', name: 49, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:43684
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:40853', name: 372, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:40853
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:33084', name: 227, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:33084
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:38583', name: 367, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:38583
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:38329', name: 237, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.190:38329
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:34619', name: 236, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:34619
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:38457', name: 219, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:38457
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:42035', name: 226, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.190:42035
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "2020-04-23-AA-fine-scale-vorticity-variance-eNATL60-BLBT02-m03-debug-daskmpi-occigen.py", line 132, in <module>
    plot_vorticity_variance(month)
NameError: name 'plot_vorticity_variance' is not defined
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:44801', name: 216, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:44801
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:36339', name: 239, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:36339
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:38358', name: 223, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:38358
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:38706', name: 230, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:38706
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:34597', name: 178, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:34597
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:41967', name: 218, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:41967
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:40676', name: 189, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:40676
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:36975', name: 170, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:36975
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:37108', name: 233, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:37108
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:39382', name: 182, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:39382
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:42788', name: 186, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:42788
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:36484', name: 184, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:36484
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:44626', name: 169, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.188:44626
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:44431', name: 187, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.188:44431
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:42262', name: 173, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:42262
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:39269', name: 185, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.188:39269
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:39716', name: 116, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:39716
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:37750', name: 112, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.185:37750
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:46139', name: 98, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.185:46139
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:35850', name: 103, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:35850
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:46628', name: 344, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.195:46628
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:34907', name: 357, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.195:34907
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:42845', name: 356, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:42845
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:33258', name: 168, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:33258
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:37485', name: 174, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:37485
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:35772', name: 238, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:35772
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.190:39567', name: 220, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.190:39567
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:46103', name: 329, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:46103
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:39394', name: 328, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:39394
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:35264', name: 113, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:35264
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:36360', name: 359, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:36360
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:46181', name: 348, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:46181
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:45063', name: 351, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:45063
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:38217', name: 339, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:38217
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:41251', name: 342, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:41251
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:32963', name: 302, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:32963
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:45650', name: 343, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:45650
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:42932', name: 306, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:42932
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:45969', name: 354, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:45969
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:39796', name: 304, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:39796
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:46198', name: 300, memory: 0, processing: 0>
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.core - INFO - Removing comms to tcp://172.30.0.193:46198
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:44377', name: 298, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:44377
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:40942', name: 292, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:40942
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:43239', name: 296, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:43239
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:37127', name: 358, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:37127
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:43870', name: 308, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:43870
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:46693', name: 290, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:46693
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:43939', name: 347, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:43939
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:33618', name: 297, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:33618
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:34746', name: 337, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:34746
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:38299', name: 291, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:38299
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:35357', name: 288, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:35357
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:45140', name: 310, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:45140
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:34846', name: 355, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:34846
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:42058', name: 341, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:42058
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:42321', name: 336, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:42321
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:46001', name: 307, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:46001
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.195:33026', name: 350, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.195:33026
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:39533', name: 293, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:39533
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:37962', name: 309, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:37962
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:34763', name: 295, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:34763
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:43341', name: 289, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:43341
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:38975', name: 181, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:38975
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:42391', name: 175, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:42391
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.193:33995', name: 303, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.193:33995
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:37533', name: 375, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:37533
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:41461', name: 190, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:41461
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:44543', name: 365, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:44543
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:41373', name: 380, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:41373
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:35261', name: 370, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:35261
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:36009', name: 364, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:36009
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:46091', name: 94, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:46091
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:42972', name: 76, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:42972
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:34045', name: 396, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:34045
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:37836', name: 383, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:37836
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:35156', name: 402, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:35156
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:34084', name: 368, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:34084
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:36899', name: 404, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:36899
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:39994', name: 401, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:39994
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:37960', name: 285, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:37960
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:43514', name: 73, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:43514
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:37882', name: 275, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:37882
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:46388', name: 67, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:46388
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:41900', name: 52, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:41900
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:34222', name: 61, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:34222
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:41162', name: 59, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:41162
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:46367', name: 57, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:46367
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:34320', name: 55, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:34320
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:41471', name: 71, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:41471
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:42744', name: 196, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:42744
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:38143', name: 215, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:38143
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:35373', name: 206, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:35373
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:35315', name: 208, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:35315
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:43734', name: 207, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:43734
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:36377', name: 199, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:36377
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:38870', name: 360, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:38870
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:34428', name: 180, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:34428
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:43993', name: 363, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:43993
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:37291', name: 177, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:37291
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:38863', name: 179, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:38863
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.188:36437', name: 171, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.188:36437
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:39648', name: 381, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:39648
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:45900', name: 413, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:45900
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:45265', name: 426, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:45265
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.196:40975', name: 361, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.196:40975
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:38860', name: 414, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:38860
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:45936', name: 425, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:45936
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:35906', name: 427, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:35906
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:34898', name: 411, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:34898
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:41503', name: 429, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:41503
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:39536', name: 449, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:39536
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:33033', name: 453, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:33033
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:42213', name: 442, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:42213
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:34014', name: 432, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:34014
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:46467', name: 97, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:46467
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:41541', name: 119, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:41541
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:36189', name: 118, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:36189
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:37948', name: 96, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:37948
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:33901', name: 101, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:33901
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:40669', name: 115, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:40669
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:40642', name: 99, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:40642
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:39199', name: 107, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:39199
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:36974', name: 100, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:36974
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:45000', name: 114, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:45000
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:41732', name: 398, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:41732
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:36337', name: 403, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:36337
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:34042', name: 110, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:34042
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:45797', name: 104, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:45797
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:44112', name: 102, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:44112
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:40879', name: 389, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:40879
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:36698', name: 406, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:36698
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:36530', name: 390, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:36530
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:33044', name: 400, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:33044
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:33995', name: 111, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:33995
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.185:33748', name: 108, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.185:33748
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:37227', name: 392, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:37227
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:37299', name: 83, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:37299
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:33311', name: 77, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:33311
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:33518', name: 90, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:33518
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:33503', name: 82, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:33503
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:33800', name: 88, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:33800
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:33298', name: 92, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:33298
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:43759', name: 386, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:43759
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:37631', name: 75, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:37631
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:43992', name: 384, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:43992
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:37332', name: 277, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:37332
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.129:38155', name: 388, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.129:38155
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:37029', name: 270, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:37029
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:39571', name: 271, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:39571
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:36466', name: 265, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:36466
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:46091', name: 269, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:46091
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:43638', name: 283, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:43638
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:44138', name: 194, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:44138
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:41168', name: 279, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:41168
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:44702', name: 210, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:44702
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:40003', name: 198, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:40003
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:45733', name: 268, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:45733
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:38094', name: 214, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:38094
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:42760', name: 212, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:42760
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:44170', name: 209, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:44170
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:40616', name: 202, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:40616
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:33408', name: 282, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:33408
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:39019', name: 273, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:39019
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:46384', name: 272, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:46384
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:38751', name: 264, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:38751
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:45745', name: 68, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:45745
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.192:46346', name: 266, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.192:46346
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:40187', name: 69, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:40187
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:35145', name: 62, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:35145
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:44832', name: 193, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:44832
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:35025', name: 54, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:35025
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:33711', name: 66, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:33711
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:39168', name: 65, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:39168
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:42356', name: 205, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:42356
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:46457', name: 213, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:46457
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:39922', name: 203, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:39922
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:45292', name: 204, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:45292
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:45642', name: 200, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:45642
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:43784', name: 63, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:43784
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:36748', name: 195, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:36748
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:43774', name: 201, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:43774
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:34262', name: 424, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:34262
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:34217', name: 417, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:34217
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:45717', name: 421, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:45717
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:41133', name: 415, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:41133
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:44852', name: 422, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:44852
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:41018', name: 70, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:41018
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:38848', name: 410, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:38848
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.189:36162', name: 192, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.189:36162
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:45212', name: 412, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:45212
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:43051', name: 60, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:43051
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:44180', name: 430, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:44180
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:34098', name: 50, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:34098
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:37335', name: 64, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:37335
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:41585', name: 48, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:41585
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:32977', name: 58, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:32977
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:36992', name: 165, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:36992
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:43373', name: 148, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:43373
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:42886', name: 146, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:42886
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:38244', name: 53, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:38244
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:44935', name: 51, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:44935
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:36056', name: 162, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:36056
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:40140', name: 431, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:40140
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:32857', name: 158, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:32857
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:35318', name: 89, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:35318
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:35711', name: 160, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:35711
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.183:35690', name: 56, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.183:35690
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:42401', name: 150, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:42401
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:45915', name: 74, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:45915
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:38846', name: 93, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:38846
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:42219', name: 78, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:42219
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:41968', name: 81, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:41968
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:46102', name: 91, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:46102
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:42318', name: 84, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:42318
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:40978', name: 87, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:40978
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:33089', name: 155, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:33089
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:40011', name: 144, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:40011
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:43489', name: 470, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:43489
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:39437', name: 79, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:39437
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:42823', name: 72, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:42823
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:37512', name: 161, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:37512
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:36947', name: 95, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:36947
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:39948', name: 468, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:39948
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:33167', name: 85, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:33167
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:44179', name: 145, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:44179
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.184:42729', name: 80, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.184:42729
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:34045', name: 163, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:34045
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:46563', name: 151, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:46563
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:33283', name: 153, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:33283
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:44992', name: 149, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:44992
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:35376', name: 167, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:35376
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:45383', name: 460, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:45383
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:36522', name: 456, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:36522
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:35059', name: 462, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:35059
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:41329', name: 157, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:41329
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:43614', name: 472, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:43614
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:34381', name: 152, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:34381
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:42595', name: 464, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:42595
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:46613', name: 474, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:46613
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:34167', name: 159, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:34167
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.187:33975', name: 154, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.187:33975
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:45815', name: 459, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:45815
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:41654', name: 478, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:41654
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:40025', name: 458, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:40025
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:42314', name: 477, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:42314
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:38067', name: 467, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:38067
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:44324', name: 473, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:44324
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:33914', name: 476, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:33914
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:36612', name: 463, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:36612
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:35159', name: 475, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:35159
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:43085', name: 457, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:43085
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.136:37056', name: 479, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.136:37056
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:36461', name: 35, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:36461
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:46622', name: 39, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:46622
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:42995', name: 43, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:42995
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:41620', name: 24, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:41620
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:39977', name: 41, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:39977
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:46288', name: 34, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:46288
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:35569', name: 44, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:35569
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:39735', name: 36, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:39735
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:46655', name: 31, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:46655
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:40934', name: 47, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:40934
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:38715', name: 45, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:38715
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:37246', name: 33, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:37246
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:40531', name: 38, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:40531
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:36571', name: 28, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:36571
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:46112', name: 37, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:46112
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:39593', name: 27, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:39593
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:35839', name: 29, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:35839
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:46229', name: 42, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:46229
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:45157', name: 25, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:45157
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:43437', name: 26, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:43437
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:34037', name: 30, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:34037
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:40096', name: 46, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:40096
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:40336', name: 40, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:40336
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.182:43037', name: 32, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.182:43037
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:35244', name: 332, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:35244
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:40242', name: 331, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:40242
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:41206', name: 316, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:41206
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:45979', name: 326, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:45979
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:38563', name: 315, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:38563
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:36750', name: 323, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:36750
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:40798', name: 312, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:40798
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:34047', name: 324, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:34047
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:34131', name: 334, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:34131
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:46257', name: 319, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:46257
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:37191', name: 327, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:37191
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:41303', name: 325, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:41303
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:41271', name: 313, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:41271
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:42806', name: 318, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:42806
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:43570', name: 441, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:43570
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:42424', name: 322, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:42424
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:33986', name: 314, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:33986
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:38164', name: 321, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:38164
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:33052', name: 333, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:33052
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.194:33130', name: 335, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.194:33130
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:39043', name: 433, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:39043
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:39514', name: 437, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:39514
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:45992', name: 443, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:45992
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:33063', name: 451, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:33063
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:36801', name: 439, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:36801
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:42085', name: 444, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:42085
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:36038', name: 455, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:36038
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:33469', name: 445, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:33469
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:39232', name: 438, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:39232
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:40957', name: 450, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:40957
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:36531', name: 436, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:36531
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:37638', name: 446, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:37638
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:35050', name: 434, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:35050
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:38304', name: 440, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:38304
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.131:37640', name: 454, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.131:37640
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:43753', name: 423, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:43753
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:38187', name: 420, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:38187
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:46259', name: 418, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:46259
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:34709', name: 416, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:34709
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.5.130:40542', name: 408, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.5.130:40542
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:46075', name: 120, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:46075
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:36217', name: 134, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:36217
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:43277', name: 139, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:43277
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:46397', name: 126, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:46397
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:42019', name: 138, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:42019
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:42187', name: 140, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:42187
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:45942', name: 132, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:45942
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:39120', name: 133, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:39120
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:41112', name: 127, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:41112
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:36673', name: 128, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:36673
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:45467', name: 125, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:45467
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:37397', name: 137, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:37397
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:34103', name: 121, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:34103
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:37933', name: 122, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:37933
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:34234', name: 135, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:34234
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:39747', name: 131, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:39747
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:37603', name: 136, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:37603
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:44714', name: 142, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:44714
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:38987', name: 141, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:38987
distributed.scheduler - INFO - Remove worker <Worker 'tcp://172.30.0.186:37509', name: 124, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.30.0.186:37509
