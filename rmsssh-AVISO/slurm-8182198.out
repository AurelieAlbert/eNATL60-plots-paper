/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:41453'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:42600'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:38445'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:46462'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:36962'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:33216'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:46026'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:34378'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:33860'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:42112'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:37596'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:35486'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:41358'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:43828'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:45984'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:37293'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:35644'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:43436'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:32817'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:41741'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:37718'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:34451'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:33983'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:42867'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:41074'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:43847'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:45768'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.8.203:40304'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:44926
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:44926
distributed.worker - INFO -              nanny at:         172.30.8.203:45768
distributed.worker - INFO -              bokeh at:         172.30.8.203:39526
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:37216
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:37216
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-w7fxuvdi
distributed.worker - INFO -              nanny at:         172.30.8.203:33983
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:         172.30.8.203:34870
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-782dn32e
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:37203
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:37203
distributed.worker - INFO -              nanny at:         172.30.8.203:41453
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:34801
distributed.worker - INFO -              bokeh at:         172.30.8.203:46246
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:34801
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO -              nanny at:         172.30.8.203:43436
distributed.worker - INFO -              bokeh at:         172.30.8.203:41559
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9up89nti
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5lw4nlxy
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:36788
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:36788
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:32880
distributed.worker - INFO -              nanny at:         172.30.8.203:42867
distributed.worker - INFO -              bokeh at:         172.30.8.203:43033
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:32880
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              nanny at:         172.30.8.203:41074
distributed.worker - INFO -              bokeh at:         172.30.8.203:46326
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wrpkks3n
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:39190
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:39190
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -              nanny at:         172.30.8.203:42600
distributed.worker - INFO -              bokeh at:         172.30.8.203:39101
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-vslyryro
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3w05f5mx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:36593
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:36593
distributed.worker - INFO -              nanny at:         172.30.8.203:37596
distributed.core - INFO - Starting established connection
distributed.worker - INFO -              bokeh at:         172.30.8.203:36051
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-76j9d_bx
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:39366
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:39366
distributed.worker - INFO -              nanny at:         172.30.8.203:41741
distributed.worker - INFO -              bokeh at:         172.30.8.203:42811
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ty7tsryq
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:34484
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:34484
distributed.worker - INFO -              nanny at:         172.30.8.203:46462
distributed.worker - INFO -              bokeh at:         172.30.8.203:40990
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-b6u26zfo
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:38616
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:38616
distributed.worker - INFO -              nanny at:         172.30.8.203:37293
distributed.worker - INFO -              bokeh at:         172.30.8.203:35606
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hbt1r3op
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:37888
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:37888
distributed.worker - INFO -              nanny at:         172.30.8.203:38445
distributed.worker - INFO -              bokeh at:         172.30.8.203:34130
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-60030khz
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:40914
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:40914
distributed.worker - INFO -              nanny at:         172.30.8.203:33860
distributed.worker - INFO -              bokeh at:         172.30.8.203:38695
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-aqics5h5
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:42274
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:42274
distributed.worker - INFO -              nanny at:         172.30.8.203:37718
distributed.worker - INFO -              bokeh at:         172.30.8.203:34505
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-uouz5ul9
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:33661
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:33661
distributed.worker - INFO -              nanny at:         172.30.8.203:34378
distributed.worker - INFO -              bokeh at:         172.30.8.203:38952
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-265ced4j
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:38628
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:38628
distributed.worker - INFO -              nanny at:         172.30.8.203:43847
distributed.worker - INFO -              bokeh at:         172.30.8.203:39163
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_thxvlqf
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:42227
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:42227
distributed.worker - INFO -              nanny at:         172.30.8.203:34451
distributed.worker - INFO -              bokeh at:         172.30.8.203:40384
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2hicc1fd
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:35051
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:35051
distributed.worker - INFO -              nanny at:         172.30.8.203:32817
distributed.worker - INFO -              bokeh at:         172.30.8.203:43285
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-_zfspc77
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:37460
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:37460
distributed.worker - INFO -              nanny at:         172.30.8.203:40304
distributed.worker - INFO -              bokeh at:         172.30.8.203:37488
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-mofjn00_
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:35391
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:35391
distributed.worker - INFO -              nanny at:         172.30.8.203:41358
distributed.worker - INFO -              bokeh at:         172.30.8.203:37770
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-janttteb
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:42540
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:42540
distributed.worker - INFO -              nanny at:         172.30.8.203:36962
distributed.worker - INFO -              bokeh at:         172.30.8.203:37269
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-by0frv9c
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:33689
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:33689
distributed.worker - INFO -              nanny at:         172.30.8.203:43828
distributed.worker - INFO -              bokeh at:         172.30.8.203:45339
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bz0q4dxr
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:33159
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:33159
distributed.worker - INFO -              nanny at:         172.30.8.203:46026
distributed.worker - INFO -              bokeh at:         172.30.8.203:46053
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-jjv844f0
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:33767
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:33767
distributed.worker - INFO -              nanny at:         172.30.8.203:45984
distributed.worker - INFO -              bokeh at:         172.30.8.203:41475
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-093324nn
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:43208
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:43208
distributed.worker - INFO -              nanny at:         172.30.8.203:33216
distributed.worker - INFO -              bokeh at:         172.30.8.203:45559
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ssptt65t
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:43447
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:43447
distributed.worker - INFO -              nanny at:         172.30.8.203:35486
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:36367
distributed.worker - INFO -              bokeh at:         172.30.8.203:37298
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:36367
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO -              nanny at:         172.30.8.203:35644
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -              bokeh at:         172.30.8.203:40535
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dq26uruk
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-k76_u5ns
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:32983
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:32983
distributed.worker - INFO -              nanny at:         172.30.8.203:42112
distributed.worker - INFO -              bokeh at:         172.30.8.203:40442
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0cr0epjg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 286 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.utils_perf - INFO - full garbage collection released 77.88 MB from 979 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 1108 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.utils_perf - INFO - full garbage collection released 145.15 MB from 492 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.utils_perf - INFO - full garbage collection released 110.31 MB from 694 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 936 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.utils_perf - INFO - full garbage collection released 77.61 MB from 3118 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.59 MB from 1157 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 59.72 MB from 3405 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.utils_perf - INFO - full garbage collection released 110.39 MB from 1818 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 64.33 MB from 6347 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.utils_perf - INFO - full garbage collection released 44.56 MB from 3134 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46496 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46490 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46492 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46842 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46546 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46826 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46508 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46498 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46832 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46844 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47152 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46730 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47132 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46830 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47126 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46764 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47124 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47352 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47354 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47338 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46710 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47120 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47456 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47192 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46468 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47134 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47422 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47172 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47470 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47250 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47300 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46674 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46772 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46824 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46928 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47332 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46612 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46658 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46908 remote=tcp://172.30.100.1:41041>
distributed.utils_perf - INFO - full garbage collection released 220.86 MB from 753 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47552 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47556 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47150 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47438 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47182 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47488 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47266 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46520 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47318 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46598 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46644 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46790 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46894 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46946 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47374 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47460 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47196 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47240 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47290 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47358 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47412 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46762 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46866 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46918 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47376 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47168 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47462 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47198 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47242 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47292 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47360 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46666 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47414 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46868 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46920 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47128 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47416 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47464 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47200 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46470 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47244 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46502 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46540 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46580 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46668 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46766 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46922 remote=tcp://172.30.100.1:41041>
distributed.utils_perf - INFO - full garbage collection released 60.41 MB from 2301 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47068 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47408 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47160 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47454 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47284 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46616 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46662 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46756 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46912 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47392 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47142 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47430 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47176 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47476 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47209 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47256 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47308 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47368 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46778 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46936 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47382 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47420 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47468 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47248 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47298 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47362 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46672 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46770 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46822 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46926 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47400 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47442 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46456 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46486 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47270 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46524 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47322 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46602 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47342 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46794 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47492 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46846 remote=tcp://172.30.100.1:41041>
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 1656 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47136 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47424 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47174 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47474 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47206 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46474 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47252 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47304 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46586 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47366 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46676 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46774 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46880 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46932 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47390 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47140 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47428 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46450 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47478 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47208 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46478 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47254 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46510 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47306 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46548 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46634 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46680 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46780 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46934 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47380 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47130 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47418 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47466 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46472 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47246 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46504 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47296 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46542 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46582 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46670 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46768 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46872 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46924 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46976 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46828 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46506 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47138 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47426 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46448 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47472 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46476 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47302 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46584 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46678 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46776 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46878 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46930 remote=tcp://172.30.100.1:41041>
distributed.utils_perf - INFO - full garbage collection released 98.44 MB from 1473 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46732 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47394 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47146 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47432 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47178 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47482 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47212 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47260 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46514 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47312 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46552 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46592 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46638 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47370 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46684 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46784 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46836 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46888 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46940 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47096 remote=tcp://172.30.100.1:41041>
distributed.utils_perf - INFO - full garbage collection released 110.37 MB from 930 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47236 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46664 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46758 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46914 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47162 remote=tcp://172.30.100.1:41041>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47122 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47458 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47238 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47356 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47410 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46760 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46916 remote=tcp://172.30.100.1:41041>
distributed.utils_perf - INFO - full garbage collection released 110.36 MB from 1479 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 339 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.06 MB from 1714 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47436 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47486 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47216 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47264 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47316 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46556 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46596 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46642 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46788 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46840 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46892 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46944 remote=tcp://172.30.100.1:41041>
distributed.utils_perf - INFO - full garbage collection released 110.54 MB from 1730 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 1186 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46734 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47396 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47148 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47434 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47180 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47484 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47214 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47262 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46516 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46594 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46640 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46786 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46838 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46890 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46942 remote=tcp://172.30.100.1:41041>
distributed.utils_perf - INFO - full garbage collection released 110.37 MB from 2430 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.35 MB from 967 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 791 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 520 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47404 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47448 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46460 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47228 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47276 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47328 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46568 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46608 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47348 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46654 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46800 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47498 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46852 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46904 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47554 remote=tcp://172.30.100.1:41041>
distributed.utils_perf - INFO - full garbage collection released 89.87 MB from 757 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 1129 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46452 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47480 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46480 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47258 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47310 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46636 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46682 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46782 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46938 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47398 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47440 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47184 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47490 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47268 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46522 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47320 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46560 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46600 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47340 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46646 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46792 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46896 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47406 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47158 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47450 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46463 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47500 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47230 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47278 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47330 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46570 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46610 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47350 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46656 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46802 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46854 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46906 remote=tcp://172.30.100.1:41041>
distributed.utils_perf - INFO - full garbage collection released 24.59 MB from 2249 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 1292 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.58 MB from 1613 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47066 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47452 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46466 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47232 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47282 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46534 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47334 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46614 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46660 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46806 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46858 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46910 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47446 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46458 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46488 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47274 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47324 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46566 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46606 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47346 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46652 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46798 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47494 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46850 remote=tcp://172.30.100.1:41041>
distributed.utils_perf - INFO - full garbage collection released 110.59 MB from 2222 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 1331 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 194 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.34 MB from 984 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 2074 reference cycles (threshold: 10.00 MB)
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47402 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47156 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47444 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47186 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47224 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47272 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47326 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46564 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46604 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47344 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46650 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46796 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:47496 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46848 remote=tcp://172.30.100.1:41041>
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.30.8.203:46900 remote=tcp://172.30.100.1:41041>
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 667 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.51 MB from 2327 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 177.81 MB from 1152 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.56 MB from 1038 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 1304 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 749 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.51 MB from 2227 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 3350 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 359 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 297 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 1095 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.40 MB from 429 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 980 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 73.32 MB from 1493 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 111.00 MB from 706 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.26 MB from 1527 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.32 MB from 305 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.80 MB from 767 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.31 MB from 286 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 789 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 98.73 MB from 715 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.59 MB from 890 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 1557 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 77.15 MB from 740 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 891 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 249 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 1855 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.59 MB from 1844 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 77.88 MB from 1505 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 977 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.61 MB from 656 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.34 MB from 931 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.59 MB from 1028 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 417 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 619 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 715 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 222 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 897 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 721 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 612 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 769 reference cycles (threshold: 10.00 MB)
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.8.211:40971
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1827, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2975, in get_data_from_worker
    max_connections=max_connections,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 533, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 139, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 114, 3, 3)
distributed.worker - INFO - Dependent not found: ('moment_chunk-d1a699f269df0c16c4dd7d89d9d0dac8', 458, 3, 3) 0 .  Asking scheduler
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.8.211:40971
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1827, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2975, in get_data_from_worker
    max_connections=max_connections,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 533, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 139, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-d73dcbf857d32d306bb31a738d03baf9', 13, 1, 16)
distributed.worker - INFO - Dependent not found: ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 53, 1, 16) 0 .  Asking scheduler
distributed.nanny - WARNING - Worker process 3495 was killed by signal 15
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.utils_perf - INFO - full garbage collection released 93.45 MB from 1156 reference cycles (threshold: 10.00 MB)
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.8.203:39190
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1827, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2975, in get_data_from_worker
    max_connections=max_connections,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 533, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 30, 2, 1)
distributed.worker - INFO - Dependent not found: ('moment_chunk-d1a699f269df0c16c4dd7d89d9d0dac8', 120, 2, 1) 0 .  Asking scheduler
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.8.203:39190
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1827, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2975, in get_data_from_worker
    max_connections=max_connections,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 533, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 52, 17, 16)
distributed.worker - INFO - Dependent not found: ('moment_chunk-d1a699f269df0c16c4dd7d89d9d0dac8', 247, 17, 16) 0 .  Asking scheduler
distributed.worker - INFO - Dependent not found: ('moment_chunk-d1a699f269df0c16c4dd7d89d9d0dac8', 208, 17, 16) 0 .  Asking scheduler
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 61, 17, 16)
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 30, 2, 1)
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 52, 17, 16)
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 61, 17, 16)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 799 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 552 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 843 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 1686 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 1046 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 2846 reference cycles (threshold: 10.00 MB)
distributed.nanny - WARNING - Restarting worker
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 459 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 1336 reference cycles (threshold: 10.00 MB)
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.8.211:40971
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1827, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2965, in get_data_from_worker
    comm = yield rpc.connect(worker)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.8.211:40971' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ae4ddcd5c18>: ConnectionRefusedError: [Errno 111] Connection refused
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 389 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 2505 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 77.61 MB from 1135 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 109.78 MB from 1573 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 1797 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 153.32 MB from 618 reference cycles (threshold: 10.00 MB)
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.9.69:41961
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1827, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2975, in get_data_from_worker
    max_connections=max_connections,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 533, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 14, 10, 8)
distributed.worker - INFO - Dependent not found: ('moment_chunk-d1a699f269df0c16c4dd7d89d9d0dac8', 59, 10, 8) 0 .  Asking scheduler
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.9.69:41961
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1827, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2975, in get_data_from_worker
    max_connections=max_connections,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 533, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - INFO - Dependent not found: ('moment_chunk-d1a699f269df0c16c4dd7d89d9d0dac8', 7, 19, 1) 0 .  Asking scheduler
distributed.worker - INFO - Dependent not found: ('moment_chunk-d1a699f269df0c16c4dd7d89d9d0dac8', 397, 19, 1) 0 .  Asking scheduler
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 99, 19, 1)
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 1, 19, 1)
distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 48.28 MB from 1548 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 82.07 MB from 684 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 1046 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.46 MB from 901 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.9.69:41961
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1827, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2975, in get_data_from_worker
    max_connections=max_connections,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 533, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 141, in convert_stream_closed_error
    raise CommClosedError("in %s: %s" % (obj, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 23, 14, 15)
distributed.worker - INFO - Dependent not found: ('moment_chunk-d1a699f269df0c16c4dd7d89d9d0dac8', 383, 9, 1) 0 .  Asking scheduler
distributed.worker - INFO - Dependent not found: ('moment_chunk-d1a699f269df0c16c4dd7d89d9d0dac8', 92, 14, 15) 0 .  Asking scheduler
distributed.worker - INFO - Can't find dependencies for key ('moment_combine-partial-435de8bbfed89d2037ceebf8590c3b70', 95, 9, 1)
distributed.utils_perf - INFO - full garbage collection released 110.59 MB from 709 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.52 MB from 1240 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.32 MB from 526 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 597 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 1385 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 1057 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 810 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 225 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 2935 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 157.03 MB from 793 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 117.86 MB from 728 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.9.69:41961
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
tornado.util.TimeoutError: Timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1827, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2965, in get_data_from_worker
    comm = yield rpc.connect(worker)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 230, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.9.69:41961' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ae4dce20cf8>: ConnectionRefusedError: [Errno 111] Connection refused
distributed.utils_perf - INFO - full garbage collection released 77.86 MB from 859 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 77.15 MB from 400 reference cycles (threshold: 10.00 MB)
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.9.69:41961
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 359, in connect
    ip, port, max_buffer_size=MAX_BUFFER_SIZE, **kwargs
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/tcpclient.py", line 280, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/tcpclient.py", line 143, in on_connect_done
    stream = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 218, in connect
    quiet_exceptions=EnvironmentError,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 371, in connect
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 139, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x2ae4dab9fb38>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1827, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 2965, in get_data_from_worker
    comm = yield rpc.connect(worker)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 864, in connect
    connection_args=self.connection_args,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 735, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 742, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 228, in connect
    _raise(error)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/core.py", line 207, in _raise
    raise IOError(msg)
OSError: Timed out trying to connect to 'tcp://172.30.9.69:41961' after 10 s: in <distributed.comm.tcp.TCPConnector object at 0x2ae4dab9fb38>: ConnectionRefusedError: [Errno 111] Connection refused
distributed.utils_perf - INFO - full garbage collection released 110.58 MB from 599 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.worker - INFO -       Start worker at:   tcp://172.30.8.203:46275
distributed.worker - INFO -          Listening to:   tcp://172.30.8.203:46275
distributed.worker - INFO -              nanny at:         172.30.8.203:42600
distributed.worker - INFO -              bokeh at:         172.30.8.203:36315
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-063vn6my
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 438 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO -         Registered to:   tcp://172.30.100.1:41041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.utils_perf - INFO - full garbage collection released 110.31 MB from 1342 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.33 MB from 678 reference cycles (threshold: 10.00 MB)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/xarray/core/merge.py:16: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 870 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 225 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.32 MB from 424 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 539 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.32 MB from 1606 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 375 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 1687 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 1624 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 1479 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 704 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 59.06 MB from 2791 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.47 MB from 1515 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.59 MB from 736 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 663.33 MB from 2947 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.35 MB from 3448 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.60 MB from 2994 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.41 MB from 1104 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 221.19 MB from 3575 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 110.28 MB from 490 reference cycles (threshold: 10.00 MB)
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:32880
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:33159
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:32983
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:33661
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:33689
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:34484
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:33767
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:34801
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:35051
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:35391
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:36593
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:36367
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:36788
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:37203
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:37216
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:37888
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:37460
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:38616
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:38628
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:39366
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:40914
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:42227
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:42540
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:42274
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:43208
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:44926
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:43447
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide
  x = np.divide(x1, x2, out)
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/array/reductions.py:387: RuntimeWarning: invalid value encountered in true_divide
  u = total / n
distributed.worker - INFO - Stopping worker at tcp://172.30.8.203:46275
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:42112'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:43828'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:37596'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:41074'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:46026'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:41358'
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:43847'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:37718'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:45984'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:43436'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:33983'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:41741'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: scheduler='tcp://172.30.100.1:41041' processes=196 cores=196>>
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py", line 907, in _run
    return self.callback()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/client.py", line 1055, in _heartbeat
    self.scheduler_comm.send({"op": "heartbeat-client"})
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/batched.py", line 119, in send
    raise CommClosedError
distributed.comm.core.CommClosedError
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:42867'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:41453'
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:34378'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:34451'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:36962'
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:33860'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:37293'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:38445'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:46462'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:40304'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:32817'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:35644'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:42600'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:33216'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:45768'
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.8.203:35486'
distributed.dask_worker - INFO - End worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  data = yaml.load(f.read()) or {}
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
slurmstepd: error: Exceeded step memory limit at some point.
slurmstepd: error: Exceeded job memory limit at some point.
